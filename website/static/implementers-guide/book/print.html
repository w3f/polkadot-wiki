<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Polkadot Parachain Host Implementers' Guide</title>
        
        <meta name="robots" content="noindex" />
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "light" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">Preamble</a></li><li class="chapter-item expanded "><a href="whence-parachains.html"><strong aria-hidden="true">1.</strong> Whence Parachains</a></li><li class="chapter-item expanded "><a href="parachains-overview.html"><strong aria-hidden="true">2.</strong> Parachains Overview</a></li><li class="chapter-item expanded "><a href="architecture.html"><strong aria-hidden="true">3.</strong> Architecture Overview</a></li><li class="chapter-item expanded "><a href="runtime/index.html"><strong aria-hidden="true">4.</strong> Runtime Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="runtime/initializer.html"><strong aria-hidden="true">4.1.</strong> Initializer Module</a></li><li class="chapter-item expanded "><a href="runtime/configuration.html"><strong aria-hidden="true">4.2.</strong> Configuration Module</a></li><li class="chapter-item expanded "><a href="runtime/paras.html"><strong aria-hidden="true">4.3.</strong> Paras Module</a></li><li class="chapter-item expanded "><a href="runtime/scheduler.html"><strong aria-hidden="true">4.4.</strong> Scheduler Module</a></li><li class="chapter-item expanded "><a href="runtime/inclusion.html"><strong aria-hidden="true">4.5.</strong> Inclusion Module</a></li><li class="chapter-item expanded "><a href="runtime/inclusioninherent.html"><strong aria-hidden="true">4.6.</strong> InclusionInherent Module</a></li><li class="chapter-item expanded "><a href="runtime/validity.html"><strong aria-hidden="true">4.7.</strong> Validity Module</a></li><li class="chapter-item expanded "><a href="runtime/router.html"><strong aria-hidden="true">4.8.</strong> Router Module</a></li></ol></li><li class="chapter-item expanded "><a href="node/index.html"><strong aria-hidden="true">5.</strong> Node Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="node/subsystems-and-jobs.html"><strong aria-hidden="true">5.1.</strong> Subsystems and Jobs</a></li><li class="chapter-item expanded "><a href="node/overseer.html"><strong aria-hidden="true">5.2.</strong> Overseer</a></li><li class="chapter-item expanded "><a href="node/backing/index.html"><strong aria-hidden="true">5.3.</strong> Backing Subsystems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="node/backing/candidate-selection.html"><strong aria-hidden="true">5.3.1.</strong> Candidate Selection</a></li><li class="chapter-item expanded "><a href="node/backing/candidate-backing.html"><strong aria-hidden="true">5.3.2.</strong> Candidate Backing</a></li><li class="chapter-item expanded "><a href="node/backing/statement-distribution.html"><strong aria-hidden="true">5.3.3.</strong> Statement Distribution</a></li><li class="chapter-item expanded "><a href="node/backing/pov-distribution.html"><strong aria-hidden="true">5.3.4.</strong> PoV Distribution</a></li></ol></li><li class="chapter-item expanded "><a href="node/availability/index.html"><strong aria-hidden="true">5.4.</strong> Availability Subsystems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="node/availability/availability-distribution.html"><strong aria-hidden="true">5.4.1.</strong> Availability Distribution</a></li><li class="chapter-item expanded "><a href="node/availability/bitfield-distribution.html"><strong aria-hidden="true">5.4.2.</strong> Bitfield Distribution</a></li><li class="chapter-item expanded "><a href="node/availability/bitfield-signing.html"><strong aria-hidden="true">5.4.3.</strong> Bitfield Signing</a></li></ol></li><li class="chapter-item expanded "><a href="node/collators/index.html"><strong aria-hidden="true">5.5.</strong> Collators</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="node/collators/collation-generation.html"><strong aria-hidden="true">5.5.1.</strong> Collation Generation</a></li><li class="chapter-item expanded "><a href="node/collators/collation-distribution.html"><strong aria-hidden="true">5.5.2.</strong> Collation Distribution</a></li></ol></li><li class="chapter-item expanded "><a href="node/validity/index.html"><strong aria-hidden="true">5.6.</strong> Validity</a></li><li class="chapter-item expanded "><a href="node/utility/index.html"><strong aria-hidden="true">5.7.</strong> Utility Subsystems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="node/utility/availability-store.html"><strong aria-hidden="true">5.7.1.</strong> Availability Store</a></li><li class="chapter-item expanded "><a href="node/utility/candidate-validation.html"><strong aria-hidden="true">5.7.2.</strong> Candidate Validation</a></li><li class="chapter-item expanded "><a href="node/utility/provisioner.html"><strong aria-hidden="true">5.7.3.</strong> Provisioner</a></li><li class="chapter-item expanded "><a href="node/utility/network-bridge.html"><strong aria-hidden="true">5.7.4.</strong> Network Bridge</a></li><li class="chapter-item expanded "><a href="node/utility/misbehavior-arbitration.html"><strong aria-hidden="true">5.7.5.</strong> Misbehavior Arbitration</a></li><li class="chapter-item expanded "><a href="node/utility/peer-set-manager.html"><strong aria-hidden="true">5.7.6.</strong> Peer Set Manager</a></li><li class="chapter-item expanded "><a href="node/utility/runtime-api.html"><strong aria-hidden="true">5.7.7.</strong> Runtime API Requests</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="types/index.html"><strong aria-hidden="true">6.</strong> Data Structures and Types</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="types/candidate.html"><strong aria-hidden="true">6.1.</strong> Candidate</a></li><li class="chapter-item expanded "><a href="types/backing.html"><strong aria-hidden="true">6.2.</strong> Backing</a></li><li class="chapter-item expanded "><a href="types/availability.html"><strong aria-hidden="true">6.3.</strong> Availability</a></li><li class="chapter-item expanded "><a href="types/overseer-protocol.html"><strong aria-hidden="true">6.4.</strong> Overseer and Subsystem Protocol</a></li><li class="chapter-item expanded "><a href="types/runtime.html"><strong aria-hidden="true">6.5.</strong> Runtime</a></li><li class="chapter-item expanded "><a href="types/chain.html"><strong aria-hidden="true">6.6.</strong> Chain</a></li><li class="chapter-item expanded "><a href="types/messages.html"><strong aria-hidden="true">6.7.</strong> Messages</a></li></ol></li><li class="chapter-item expanded "><a href="glossary.html">Glossary</a></li><li class="chapter-item expanded affix "><a href="further-reading.html">Further Reading</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">The Polkadot Parachain Host Implementers' Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#preamble" id="preamble">Preamble</a></h1>
<p>This document aims to describe the purpose, functionality, and implementation of a host for Polkadot's <em>parachains</em>. It is not for the implementor of a specific parachain but rather for the implementor of the Parachain Host, which provides security and advancement for constituent parachains. In practice, this is for the implementors of Polkadot.</p>
<p>There are a number of other documents describing the research in more detail. All referenced documents will be linked here and should be read alongside this document for the best understanding of the full picture. However, this is the only document which aims to describe key aspects of Polkadot's particular instantiation of much of that research down to low-level technical details and software architecture.</p>
<h1><a class="header" href="#whence-parachains" id="whence-parachains">Whence Parachains</a></h1>
<p>Parachains are the solution to a problem. As with any solution, it cannot be understood without first understanding the problem. So let's start by going over the issues faced by blockchain technology that led to us beginning to explore the design space for something like parachains.</p>
<h2><a class="header" href="#issue-1-scalability" id="issue-1-scalability">Issue 1: Scalability</a></h2>
<p>It became clear a few years ago that the transaction throughput of simple Proof-of-Work (PoW) blockchains such as Bitcoin, Ethereum, and myriad others was simply too low.</p>
<blockquote>
<p>TODO: what if there were more blockchains, etc.</p>
</blockquote>
<p>Proof-of-Stake (PoS) systems can accomplish higher throughput than PoW blockchains. PoS systems are secured by bonded capital as opposed to spent effort - liquidity opportunity cost vs. burning electricity. The way they work is by selecting a set of validators with known economic identity who lock up tokens in exchange for earning the right to &quot;validate&quot; or participate in the consensus process. If they are found to carry out that process wrongly, they will be slashed, meaning some or all of the locked tokens will be burned. This provides a strong disincentive in the direction of misbehavior.</p>
<p>Since the consensus protocol doesn't revolve around wasting effort, block times and agreement can occur much faster. Solutions to PoW challenges don't have to be found before a block can be authored, so the overhead of authoring a block is reduced to only the costs of creating and distributing the block.</p>
<p>However, consensus on a PoS chain requires full agreement of 2/3+ of the validator set for everything that occurs at Layer 1: all logic which is carried out as part of the blockchain's state machine. This means that everybody still needs to check everything. Furthermore, validators may have different views of the system based on the information that they receive over an asynchronous network, making agreement on the latest state more difficult.</p>
<p>Parachains are an example of a <strong>sharded</strong> protocol. Sharding is a concept borrowed from traditional database architecture. Rather than requiring every participant to check every transaction, we require each participant to check some subset of transactions, with enough redundancy baked in that byzantine (arbitrarily malicious) participants can't sneak in invalid transactions - at least not without being detected and getting slashed, with those transactions reverted.</p>
<p>Sharding and Proof-of-Stake in coordination with each other allow a parachain host to provide full security on many parachains, even without all participants checking all state transitions.</p>
<blockquote>
<p>TODO: note about network effects &amp; bridging</p>
</blockquote>
<h2><a class="header" href="#issue-2-flexibility--specialization" id="issue-2-flexibility--specialization">Issue 2: Flexibility / Specialization</a></h2>
<p>&quot;dumb&quot; VMs don't give you the flexibility. Any engineer knows that being able to specialize on a problem gives them and their users more <em>leverage</em>.</p>
<blockquote>
<p>TODO: expand on leverage</p>
</blockquote>
<p>Having recognized these issues, we set out to find a solution to these problems, which could allow developers to create and deploy purpose-built blockchains unified under a common source of security, with the capability of message-passing between them; a <em>heterogeneous sharding solution</em>, which we have come to know as <strong>Parachains</strong>.</p>
<h1><a class="header" href="#parachains-overview" id="parachains-overview">Parachains Overview</a></h1>
<p>This section aims to describe, at a high level, the architecture, actors, and Subsystems involved in the implementation of parachains. It also illuminates certain subtleties and challenges faced in the design and implementation of those Subsystems. Our goal is to carry a parachain block from authoring to secure inclusion, and define a process which can be carried out repeatedly and in parallel for many different parachains to extend them over time. Understanding of the high-level approach taken here is important to provide context for the proposed architecture further on.</p>
<p>The Parachain Host is a blockchain, known as the relay-chain, and the actors which provide security and inputs to the blockchain.</p>
<p>First, it's important to go over the main actors we have involved in the parachain host.</p>
<ol>
<li>Validators. These nodes are responsible for validating proposed parachain blocks. They do so by checking a Proof-of-Validity (PoV) of the block and ensuring that the PoV remains available. They put financial capital down as &quot;skin in the game&quot; which can be slashed (destroyed) if they are proven to have misvalidated.</li>
<li>Collators. These nodes are responsible for creating the Proofs-of-Validity that validators know how to check. Creating a PoV typically requires familiarity with the transaction format and block authoring rules of the parachain, as well as having access to the full state of the parachain.</li>
<li>Fishermen. These are user-operated, permissionless nodes whose goal is to catch misbehaving validators in exchange for a bounty. Collators and validators can behave as Fishermen too. Fishermen aren't necessary for security, and aren't covered in-depth by this document.</li>
</ol>
<p>This implies a simple pipeline where collators send validators parachain blocks and their requisite PoV to check. Then, validators validate the block using the PoV, signing statements which describe either the positive or negative outcome, and with enough positive statements, the block can be noted on the relay-chain. Negative statements are not a veto but will lead to a dispute, with those on the wrong side being slashed. If another validator later detects that a validator or group of validators incorrectly signed a statement claiming a block was valid, then those validators will be <em>slashed</em>, with the checker receiving a bounty.</p>
<p>However, there is a problem with this formulation. In order for another validator to check the previous group of validators' work after the fact, the PoV must remain <em>available</em> so the other validator can fetch it in order to check the work. The PoVs are expected to be too large to include in the blockchain directly, so we require an alternate <em>data availability</em> scheme which requires validators to prove that the inputs to their work will remain available, and so their work can be checked. Empirical tests tell us that many PoVs may be between 1 and 10MB during periods of heavy load.</p>
<p>Here is a description of the Inclusion Pipeline: the path a parachain block (or parablock, for short) takes from creation to inclusion:</p>
<ol>
<li>Validators are selected and assigned to parachains by the Validator Assignment routine.</li>
<li>A collator produces the parachain block, which is known as a parachain candidate or candidate, along with a PoV for the candidate.</li>
<li>The collator forwards the candidate and PoV to validators assigned to the same parachain via the <a href="node/collators/collation-distribution.html">Collation Distribution subsystem</a>.</li>
<li>The validators assigned to a parachain at a given point in time participate in the <a href="node/backing/candidate-backing.html">Candidate Backing subsystem</a> to validate candidates that were put forward for validation. Candidates which gather enough signed validity statements from validators are considered &quot;backable&quot;. Their backing is the set of signed validity statements.</li>
<li>A relay-chain block author, selected by BABE, can note up to one (1) backable candidate for each parachain to include in the relay-chain block alongside its backing. A backable candidate once included in the relay-chain is considered backed in that fork of the relay-chain.</li>
<li>Once backed in the relay-chain, the parachain candidate is considered to be &quot;pending availability&quot;. It is not considered to be included as part of the parachain until it is proven available.</li>
<li>In the following relay-chain blocks, validators will participate in the <a href="node/availability/availability-distribution.html">Availability Distribution subsystem</a> to ensure availability of the candidate. Information regarding the availability of the candidate will be noted in the subsequent relay-chain blocks.</li>
<li>Once the relay-chain state machine has enough information to consider the candidate's PoV as being available, the candidate is considered to be part of the parachain and is graduated to being a full parachain block, or parablock for short.</li>
</ol>
<p>Note that the candidate can fail to be included in any of the following ways:</p>
<ul>
<li>The collator is not able to propagate the candidate to any validators assigned to the parachain.</li>
<li>The candidate is not backed by validators participating in the Candidate Backing Subsystem.</li>
<li>The candidate is not selected by a relay-chain block author to be included in the relay chain</li>
<li>The candidate's PoV is not considered as available within a timeout and is discarded from the relay chain.</li>
</ul>
<p>This process can be divided further down. Steps 2 &amp; 3 relate to the work of the collator in collating and distributing the candidate to validators via the Collation Distribution Subsystem. Steps 3 &amp; 4 relate to the work of the validators in the Candidate Backing Subsystem and the block author (itself a validator) to include the block into the relay chain. Steps 6, 7, and 8 correspond to the logic of the relay-chain state-machine (otherwise known as the Runtime) used to fully incorporate the block into the chain. Step 7 requires further work on the validators' parts to participate in the Availability Distribution Subsystem and include that information into the relay chain for step 8 to be fully realized.</p>
<p>This brings us to the second part of the process. Once a parablock is considered available and part of the parachain, it is still &quot;pending approval&quot;. At this stage in the pipeline, the parablock has been backed by a majority of validators in the group assigned to that parachain, and its data has been guaranteed available by the set of validators as a whole. Once it's considered available, the host will even begin to accept children of that block. At this point, we can consider the parablock as having been tentatively included in the parachain, although more confirmations are desired. However, the validators in the parachain-group (known as the &quot;Parachain Validators&quot; for that parachain) are sampled from a validator set which contains some proportion of byzantine, or arbitrarily malicious members. This implies that the Parachain Validators for some parachain may be majority-dishonest, which means that (secondary) approval checks must be done on the block before it can be considered approved. This is necessary only because the Parachain Validators for a given parachain are sampled from an overall validator set which is assumed to be up to &lt;1/3 dishonest - meaning that there is a chance to randomly sample Parachain Validators for a parachain that are majority or fully dishonest and can back a candidate wrongly. The Approval Process allows us to detect such misbehavior after-the-fact without allocating more Parachain Validators and reducing the throughput of the system. A parablock's failure to pass the approval process will invalidate the block as well as all of its descendents. However, only the validators who backed the block in question will be slashed, not the validators who backed the descendents.</p>
<p>The Approval Process looks like this:</p>
<ol>
<li>Parablocks that have been included by the Inclusion Pipeline are pending approval for a time-window known as the secondary checking window.</li>
<li>During the secondary-checking window, validators randomly self-select to perform secondary checks on the parablock.</li>
<li>These validators, known in this context as secondary checkers, acquire the parablock and its PoV, and re-run the validation function.</li>
<li>The secondary checkers submit the result of their checks to the relay chain. Contradictory results lead to escalation, where even more secondary checkers are selected and the secondary-checking window is extended.</li>
<li>At the end of the Approval Process, the parablock is either Approved or it is rejected. More on the rejection process later.</li>
</ol>
<p>These two pipelines sum up the sequence of events necessary to extend and acquire full security on a Parablock. Note that the Inclusion Pipeline must conclude for a specific parachain before a new block can be accepted on that parachain. After inclusion, the Approval Process kicks off, and can be running for many parachain blocks at once.</p>
<p>Reiterating the lifecycle of a candidate:</p>
<ol>
<li>Candidate: put forward by a collator to a validator.</li>
<li>Seconded: put forward by a validator to other validators</li>
<li>Backable: validity attested to by a majority of assigned validators</li>
<li>Backed: Backable &amp; noted in a fork of the relay-chain.</li>
<li>Pending availability: Backed but not yet considered available.</li>
<li>Included: Backed and considered available.</li>
<li>Accepted: Backed, available, and undisputed</li>
</ol>
<blockquote>
<p>TODO Diagram: Inclusion Pipeline &amp; Approval Subsystems interaction</p>
</blockquote>
<p>It is also important to take note of the fact that the relay-chain is extended by BABE, which is a forkful algorithm. That means that different block authors can be chosen at the same time, and may not be building on the same block parent. Furthermore, the set of validators is not fixed, nor is the set of parachains. And even with the same set of validators and parachains, the validators' assignments to parachains is flexible. This means that the architecture proposed in the next chapters must deal with the variability and multiplicity of the network state.</p>
<pre><code class="language-text">
   ....... Validator Group 1 ..........
   .                                  .
   .         (Validator 4)            .
   .  (Validator 1) (Validator 2)     .
   .         (Validator 5)            .
   .                                  .
   ..........Building on C  ...........        ........ Validator Group 2 ...........
            +----------------------+           .                                    .
            |    Relay Block C     |           .           (Validator 7)            .
            +----------------------+           .    ( Validator 3) (Validator 6)    .
                            \                  .                                    .
                             \                 ......... Building on B  .............
                              \
                      +----------------------+
                      |  Relay Block B       |
                      +----------------------+
                              |
                      +----------------------+
                      |  Relay Block A       |
                      +----------------------+

</code></pre>
<p>In this example, group 1 has received block C while the others have not due to network asynchrony. Now, a validator from group 2 may be able to build another block on top of B, called C'. Assume that afterwards, some validators become aware of both C and C', while others remain only aware of one.</p>
<pre><code class="language-text">   ....... Validator Group 1 ..........      ........ Validator Group 2 ...........
   .                                  .      .                                    .
   .  (Validator 4) (Validator 1)     .      .    (Validator 7) (Validator 6)     .
   .                                  .      .                                    .
   .......... Building on C  ..........      ......... Building on C' .............


   ....... Validator Group 3 ..........
   .                                  .
   .   (Validator 2) (Validator 3)    .
   .        (Validator 5)             .
   .                                  .
   ....... Building on C and C' .......

            +----------------------+         +----------------------+
            |    Relay Block C     |         |    Relay Block C'    |
            +----------------------+         +----------------------+
                            \                 /
                             \               /
                              \             /
                      +----------------------+
                      |  Relay Block B       |
                      +----------------------+
                              |
                      +----------------------+
                      |  Relay Block A       |
                      +----------------------+
</code></pre>
<p>Those validators that are aware of many competing heads must be aware of the work happening on each one. They may contribute to some or a full extent on both. It is possible that due to network asynchrony two forks may grow in parallel for some time, although in the absence of an adversarial network this is unlikely in the case where there are validators who are aware of both chain heads.</p>
<h1><a class="header" href="#architecture-overview" id="architecture-overview">Architecture Overview</a></h1>
<p>Our Parachain Host includes a blockchain known as the relay-chain. A blockchain is a Directed Acyclic Graph (DAG) of state transitions, where every block can be considered to be the head of a linked-list (known as a &quot;chain&quot; or &quot;fork&quot;) with a cumulative state which is determined by applying the state transition of each block in turn. All paths through the DAG terminate at the Genesis Block. In fact, the blockchain is a tree, since each block can have only one parent.</p>
<pre><code class="language-text">          +----------------+     +----------------+
          |    Block 4     |     | Block 5        |
          +----------------+     +----------------+
                        \           /
                         V         V
                      +---------------+
                      |    Block 3    |
                      +---------------+
                              |
                              V
                     +----------------+     +----------------+
                     |    Block 1     |     |   Block 2      |
                     +----------------+     +----------------+
                                  \            /
                                   V          V
                                +----------------+
                                |    Genesis     |
                                +----------------+
</code></pre>
<p>A blockchain network is comprised of nodes. These nodes each have a view of many different forks of a blockchain and must decide which forks to follow and what actions to take based on the forks of the chain that they are aware of.</p>
<p>So in specifying an architecture to carry out the functionality of a Parachain Host, we have to answer two categories of questions:</p>
<ol>
<li>What is the state-transition function of the blockchain? What is necessary for a transition to be considered valid, and what information is carried within the implicit state of a block?</li>
<li>Being aware of various forks of the blockchain as well as global private state such as a view of the current time, what behaviors should a node undertake? What information should a node extract from the state of which forks, and how should that information be used?</li>
</ol>
<p>The first category of questions will be addressed by the Runtime, which defines the state-transition logic of the chain. Runtime logic only has to focus on the perspective of one chain, as each state has only a single parent state.</p>
<p>The second category of questions addressed by Node-side behavior. Node-side behavior defines all activities that a node undertakes, given its view of the blockchain/block-DAG. Node-side behavior can take into account all or many of the forks of the blockchain, and only conditionally undertake certain activities based on which forks it is aware of, as well as the state of the head of those forks.</p>
<pre><code class="language-text">
                     __________________________________
                    /                                  \
                    |            Runtime               |
                    |                                  |
                    \_________(Runtime API )___________/
                                |       ^
                                V       |
               +----------------------------------------------+
               |                                              |
               |                   Node                       |
               |                                              |
               |                                              |
               +----------------------------------------------+
                                   +  +
                                   |  |
               --------------------+  +------------------------
                                 Transport
               ------------------------------------------------

</code></pre>
<p>It is also helpful to divide Node-side behavior into two further categories: Networking and Core. Networking behaviors relate to how information is distributed between nodes. Core behaviors relate to internal work that a specific node does. These two categories of behavior often interact, but can be heavily abstracted from each other. Core behaviors care that information is distributed and received, but not the internal details of how distribution and receipt function. Networking behaviors act on requests for distribution or fetching of information, but are not concerned with how the information is used afterwards. This allows us to create clean boundaries between Core and Networking activities, improving the modularity of the code.</p>
<pre><code class="language-text">          ___________________                    ____________________
         /       Core        \                  /     Networking     \
         |                   |  Send &quot;Hello&quot;    |                    |
         |                   |-  to &quot;foo&quot;   ---&gt;|                    |
         |                   |                  |                    |
         |                   |                  |                    |
         |                   |                  |                    |
         |                   |    Got &quot;World&quot;   |                    |
         |                   |&lt;--  from &quot;bar&quot; --|                    |
         |                   |                  |                    |
         \___________________/                  \____________________/
                                                   ______| |______
                                                   ___Transport___

</code></pre>
<p>Node-side behavior is split up into various subsystems. Subsystems are long-lived workers that perform a particular category of work. Subsystems can communicate with each other, and do so via an <a href="node/overseer.html">Overseer</a> that prevents race conditions.</p>
<p>Runtime logic is divided up into Modules and APIs. Modules encapsulate particular behavior of the system. Modules consist of storage, routines, and entry-points. Routines are invoked by entry points, by other modules, upon block initialization or closing. Routines can read and alter the storage of the module. Entry-points are the means by which new information is introduced to a module and can limit the origins (user, root, parachain) that they accept being called by. Each block in the blockchain contains a set of Extrinsics. Each extrinsic targets a a specific entry point to trigger and which data should be passed to it. Runtime APIs provide a means for Node-side behavior to extract meaningful information from the state of a single fork.</p>
<p>These two aspects of the implementation are heavily dependent on each other. The Runtime depends on Node-side behavior to author blocks, and to include Extrinsics which trigger the correct entry points. The Node-side behavior relies on Runtime APIs to extract information necessary to determine which actions to take.</p>
<h1><a class="header" href="#runtime-architecture" id="runtime-architecture">Runtime Architecture</a></h1>
<p>It's clear that we want to separate different aspects of the runtime logic into different modules. Modules define their own storage, routines, and entry-points. They also define initialization and finalization logic.</p>
<p>Due to the (lack of) guarantees provided by a particular blockchain-runtime framework, there is no defined or dependable order in which modules' initialization or finalization logic will run. Supporting this blockchain-runtime framework is important enough to include that same uncertainty in our model of runtime modules in this guide. Furthermore, initialization logic of modules can trigger the entry-points or routines of other modules. This is one architectural pressure against dividing the runtime logic into multiple modules. However, in this case the benefits of splitting things up outweigh the costs, provided that we take certain precautions against initialization and entry-point races.</p>
<p>We also expect, although it's beyond the scope of this guide, that these runtime modules will exist alongside various other modules. This has two facets to consider. First, even if the modules that we describe here don't invoke each others' entry points or routines during initialization, we still have to protect against those other modules doing that. Second, some of those modules are expected to provide governance capabilities for the chain. Configuration exposed by parachain-host modules is mostly for the benefit of these governance modules, to allow the operators or community of the chain to tweak parameters.</p>
<p>The runtime's primary roles to manage scheduling and updating of parachains and parathreads, as well as handling misbehavior reports and slashing. This guide doesn't focus on how parachains or parathreads are registered, only that they are. Also, this runtime description assumes that validator sets are selected somehow, but doesn't assume any other details than a periodic <em>session change</em> event. Session changes give information about the incoming validator set and the validator set of the following session.</p>
<p>The runtime also serves another role, which is to make data available to the Node-side logic via Runtime APIs. These Runtime APIs should be sufficient for the Node-side code to author blocks correctly.</p>
<p>There is some functionality of the relay chain relating to parachains that we also consider beyond the scope of this document. In particular, all modules related to how parachains are registered aren't part of this guide, although we do provide routines that should be called by the registration process.</p>
<p>We will split the logic of the runtime up into these modules:</p>
<ul>
<li>Initializer: manage initialization order of the other modules.</li>
<li>Configuration: manage configuration and configuration updates in a non-racy manner.</li>
<li>Paras: manage chain-head and validation code for parachains and parathreads.</li>
<li>Scheduler: manages parachain and parathread scheduling as well as validator assignments.</li>
<li>Inclusion: handles the inclusion and availability of scheduled parachains and parathreads.</li>
<li>Validity: handles secondary checks and dispute resolution for included, available parablocks.</li>
</ul>
<p>The <a href="runtime/initializer.html">Initializer module</a> is special - it's responsible for handling the initialization logic of the other modules to ensure that the correct initialization order and related invariants are maintained. The other modules won't specify a on-initialize logic, but will instead expose a special semi-private routine that the initialization module will call. The other modules are relatively straightforward and perform the roles described above.</p>
<p>The Parachain Host operates under a changing set of validators. Time is split up into periodic sessions, where each session brings a potentially new set of validators. Sessions are buffered by one, meaning that the validators of the upcoming session are fixed and always known. Parachain Host runtime modules need to react to changes in the validator set, as it will affect the runtime logic for processing candidate backing, availability bitfields, and misbehavior reports. The Parachain Host modules can't determine ahead-of-time exactly when session change notifications are going to happen within the block (note: this depends on module initialization order again - better to put session before parachains modules). Ideally, session changes are always handled before initialization. It is clearly a problem if we compute validator assignments to parachains during initialization and then the set of validators changes. In the best case, we can recognize that re-initialization needs to be done. In the worst case, bugs would occur.</p>
<p>There are 3 main ways that we can handle this issue:</p>
<ol>
<li>Establish an invariant that session change notifications always happen after initialization. This means that when we receive a session change notification before initialization, we call the initialization routines before handling the session change.</li>
<li>Require that session change notifications always occur before initialization. Brick the chain if session change notifications ever happen after initialization.</li>
<li>Handle both the before and after cases.</li>
</ol>
<p>Although option 3 is the most comprehensive, it runs counter to our goal of simplicity. Option 1 means requiring the runtime to do redundant work at all sessions and will also mean, like option 3, that designing things in such a way that initialization can be rolled back and reapplied under the new environment. That leaves option 2, although it is a &quot;nuclear&quot; option in a way and requires us to constrain the parachain host to only run in full runtimes with a certain order of operations.</p>
<p>So the other role of the initializer module is to forward session change notifications to modules in the initialization order, throwing an unrecoverable error if the notification is received after initialization. Session change is the point at which the <a href="runtime/configuration.html">Configuration Module</a> updates the configuration. Most of the other modules will handle changes in the configuration during their session change operation, so the initializer should provide both the old and new configuration to all the other
modules alongside the session change notification. This means that a session change notification should consist of the following data:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct SessionChangeNotification {
 // The new validators in the session.
 validators: Vec&lt;ValidatorId&gt;,
 // The validators for the next session.
 queued: Vec&lt;ValidatorId&gt;,
 // The configuration before handling the session change.
 prev_config: HostConfiguration,
 // The configuration after handling the session change.
 new_config: HostConfiguration,
 // A secure randomn seed for the session, gathered from BABE.
 random_seed: [u8; 32],
 // The session index of the beginning session.
 session_index: SessionIndex,
}
<span class="boring">}
</span></code></pre></pre>
<blockquote>
<p>REVIEW: other options? arguments in favor of going for options 1 or 3 instead of 2. we could do a &quot;soft&quot; version of 2 where we note that the chain is potentially broken due to bad initialization order
TODO Diagram: order of runtime operations (initialization, session change)</p>
</blockquote>
<h1><a class="header" href="#initializer-module" id="initializer-module">Initializer Module</a></h1>
<p>This module is responsible for initializing the other modules in a deterministic order. It also has one other purpose as described above: accepting and forwarding session change notifications.</p>
<h2><a class="header" href="#storage" id="storage">Storage</a></h2>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>HasInitialized: bool
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#initialization" id="initialization">Initialization</a></h2>
<p>The other modules are initialized in this order:</p>
<ol>
<li>Configuration</li>
<li>Paras</li>
<li>Scheduler</li>
<li>Inclusion</li>
<li>Validity.</li>
<li>Router.</li>
</ol>
<p>The <a href="runtime/configuration.html">Configuration Module</a> is first, since all other modules need to operate under the same configuration as each other. It would lead to inconsistency if, for example, the scheduler ran first and then the configuration was updated before the Inclusion module.</p>
<p>Set <code>HasInitialized</code> to true.</p>
<h2><a class="header" href="#session-change" id="session-change">Session Change</a></h2>
<p>If <code>HasInitialized</code> is true, throw an unrecoverable error (panic).
Otherwise, forward the session change notification to other modules in initialization order.</p>
<h2><a class="header" href="#finalization" id="finalization">Finalization</a></h2>
<p>Finalization order is less important in this case than initialization order, so we finalize the modules in the reverse order from initialization.</p>
<p>Set <code>HasInitialized</code> to false.</p>
<h1><a class="header" href="#configuration-module" id="configuration-module">Configuration Module</a></h1>
<p>This module is responsible for managing all configuration of the parachain host in-flight. It provides a central point for configuration updates to prevent races between configuration changes and parachain-processing logic. Configuration can only change during the session change routine, and as this module handles the session change notification first it provides an invariant that the configuration does not change throughout the entire session. Both the <a href="runtime/scheduler.html">scheduler</a> and <a href="runtime/inclusion.html">inclusion</a> modules rely on this invariant to ensure proper behavior of the scheduler.</p>
<p>The configuration that we will be tracking is the <a href="runtime/../types/runtime.html#host-configuration"><code>HostConfiguration</code></a> struct.</p>
<h2><a class="header" href="#storage-1" id="storage-1">Storage</a></h2>
<p>The configuration module is responsible for two main pieces of storage.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// The current configuration to be used.
Configuration: HostConfiguration;
/// A pending configuration to be applied on session change.
PendingConfiguration: Option&lt;HostConfiguration&gt;;
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#session-change-1" id="session-change-1">Session change</a></h2>
<p>The session change routine for the Configuration module is simple. If the <code>PendingConfiguration</code> is <code>Some</code>, take its value and set <code>Configuration</code> to be equal to it. Reset <code>PendingConfiguration</code> to <code>None</code>.</p>
<h2><a class="header" href="#routines" id="routines">Routines</a></h2>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// Get the host configuration.
pub fn configuration() -&gt; HostConfiguration {
  Configuration::get()
}

/// Updating the pending configuration to be applied later.
fn update_configuration(f: impl FnOnce(&amp;mut HostConfiguration)) {
  PendingConfiguration::mutate(|pending| {
    let mut x = pending.unwrap_or_else(Self::configuration);
    f(&amp;mut x);
    *pending = Some(x);
  })
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#entry-points" id="entry-points">Entry-points</a></h2>
<p>The Configuration module exposes an entry point for each configuration member. These entry-points accept calls only from governance origins. These entry-points will use the <code>update_configuration</code> routine to update the specific configuration field.</p>
<h1><a class="header" href="#paras-module" id="paras-module">Paras Module</a></h1>
<p>The Paras module is responsible for storing information on parachains and parathreads. Registered parachains and parathreads cannot change except at session boundaries. This is primarily to ensure that the number of bits required for the availability bitfields does not change except at session boundaries.</p>
<p>It's also responsible for managing parachain validation code upgrades as well as maintaining availability of old parachain code and its pruning.</p>
<h2><a class="header" href="#storage-2" id="storage-2">Storage</a></h2>
<p>Utility structs:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>// the two key times necessary to track for every code replacement.
pub struct ReplacementTimes {
 /// The relay-chain block number that the code upgrade was expected to be activated.
 /// This is when the code change occurs from the para's perspective - after the
 /// first parablock included with a relay-parent with number &gt;= this value.
 expected_at: BlockNumber,
 /// The relay-chain block number at which the parablock activating the code upgrade was
 /// actually included. This means considered included and available, so this is the time at which
 /// that parablock enters the acceptance period in this fork of the relay-chain.
 activated_at: BlockNumber,
}

/// Metadata used to track previous parachain validation code that we keep in
/// the state.
pub struct ParaPastCodeMeta {
 // Block numbers where the code was expected to be replaced and where the code
 // was actually replaced, respectively. The first is used to do accurate lookups
 // of historic code in historic contexts, whereas the second is used to do
 // pruning on an accurate timeframe. These can be used as indices
 // into the `PastCode` map along with the `ParaId` to fetch the code itself.
 upgrade_times: Vec&lt;ReplacementTimes&gt;,
 // This tracks the highest pruned code-replacement, if any.
 last_pruned: Option&lt;BlockNumber&gt;,
}

enum UseCodeAt {
 // Use the current code.
 Current,
 // Use the code that was replaced at the given block number.
 ReplacedAt(BlockNumber),
}

struct ParaGenesisArgs {
  /// The initial head-data to use.
  genesis_head: HeadData,
  /// The validation code to start with.
  validation_code: ValidationCode,
  /// True if parachain, false if parathread.
  parachain: bool,
}
<span class="boring">}
</span></code></pre></pre>
<p>Storage layout:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// All parachains. Ordered ascending by ParaId. Parathreads are not included.
Parachains: Vec&lt;ParaId&gt;,
/// All parathreads.
Parathreads: map ParaId =&gt; Option&lt;()&gt;,
/// The head-data of every registered para.
Heads: map ParaId =&gt; Option&lt;HeadData&gt;;
/// The validation code of every live para.
ValidationCode: map ParaId =&gt; Option&lt;ValidationCode&gt;;
/// Actual past code, indicated by the para id as well as the block number at which it became outdated.
PastCode: map (ParaId, BlockNumber) =&gt; Option&lt;ValidationCode&gt;;
/// Past code of parachains. The parachains themselves may not be registered anymore,
/// but we also keep their code on-chain for the same amount of time as outdated code
/// to keep it available for secondary checkers.
PastCodeMeta: map ParaId =&gt; ParaPastCodeMeta;
/// Which paras have past code that needs pruning and the relay-chain block at which the code was replaced.
/// Note that this is the actual height of the included block, not the expected height at which the
/// code upgrade would be applied, although they may be equal.
/// This is to ensure the entire acceptance period is covered, not an offset acceptance period starting
/// from the time at which the parachain perceives a code upgrade as having occurred.
/// Multiple entries for a single para are permitted. Ordered ascending by block number.
PastCodePruning: Vec&lt;(ParaId, BlockNumber)&gt;;
/// The block number at which the planned code change is expected for a para.
/// The change will be applied after the first parablock for this ID included which executes
/// in the context of a relay chain block with a number &gt;= `expected_at`.
FutureCodeUpgrades: map ParaId =&gt; Option&lt;BlockNumber&gt;;
/// The actual future code of a para.
FutureCode: map ParaId =&gt; Option&lt;ValidationCode&gt;;

/// Upcoming paras (chains and threads). These are only updated on session change. Corresponds to an
/// entry in the upcoming-genesis map.
UpcomingParas: Vec&lt;ParaId&gt;;
/// Upcoming paras instantiation arguments.
UpcomingParasGenesis: map ParaId =&gt; Option&lt;ParaGenesisArgs&gt;;
/// Paras that are to be cleaned up at the end of the session.
OutgoingParas: Vec&lt;ParaId&gt;;
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#session-change-2" id="session-change-2">Session Change</a></h2>
<ol>
<li>Clean up outgoing paras. This means removing the entries under <code>Heads</code>, <code>ValidationCode</code>, <code>FutureCodeUpgrades</code>, and <code>FutureCode</code>. An according entry should be added to <code>PastCode</code>, <code>PastCodeMeta</code>, and <code>PastCodePruning</code> using the outgoing <code>ParaId</code> and removed <code>ValidationCode</code> value. This is because any outdated validation code must remain available on-chain for a determined amount of blocks, and validation code outdated by de-registering the para is still subject to that invariant.</li>
<li>Apply all incoming paras by initializing the <code>Heads</code> and <code>ValidationCode</code> using the genesis parameters.</li>
<li>Amend the <code>Parachains</code> list to reflect changes in registered parachains.</li>
<li>Amend the <code>Parathreads</code> set to reflect changes in registered parathreads.</li>
</ol>
<h2><a class="header" href="#initialization-1" id="initialization-1">Initialization</a></h2>
<ol>
<li>Do pruning based on all entries in <code>PastCodePruning</code> with <code>BlockNumber &lt;= now</code>. Update the corresponding <code>PastCodeMeta</code> and <code>PastCode</code> accordingly.</li>
</ol>
<h2><a class="header" href="#routines-1" id="routines-1">Routines</a></h2>
<ul>
<li>
<p><code>schedule_para_initialize(ParaId, ParaGenesisArgs)</code>: schedule a para to be initialized at the next session.</p>
</li>
<li>
<p><code>schedule_para_cleanup(ParaId)</code>: schedule a para to be cleaned up at the next session.</p>
</li>
<li>
<p><code>schedule_code_upgrade(ParaId, ValidationCode, expected_at: BlockNumber)</code>: Schedule a future code upgrade of the given parachain, to be applied after inclusion of a block of the same parachain executed in the context of a relay-chain block with number &gt;= <code>expected_at</code>.</p>
</li>
<li>
<p><code>note_new_head(ParaId, HeadData, BlockNumber)</code>: note that a para has progressed to a new head, where the new head was executed in the context of a relay-chain block with given number. This will apply pending code upgrades based on the block number provided.</p>
</li>
<li>
<p><code>validation_code_at(ParaId, at: BlockNumber, assume_intermediate: Option&lt;BlockNumber&gt;)</code>: Fetches the validation code to be used when validating a block in the context of the given relay-chain height. A second block number parameter may be used to tell the lookup to proceed as if an intermediate parablock has been included at the given relay-chain height. This may return past, current, or (with certain choices of <code>assume_intermediate</code>) future code. <code>assume_intermediate</code>, if provided, must be before <code>at</code>. If the validation code has been pruned, this will return <code>None</code>.</p>
</li>
<li>
<p><code>is_parathread(ParaId) -&gt; bool</code>: Returns true if the para ID references any live parathread.</p>
</li>
<li>
<p><code>last_code_upgrade(id: ParaId, include_future: bool) -&gt; Option&lt;BlockNumber&gt;</code>: The block number of the last scheduled upgrade of the requested para. Includes future upgrades if the flag is set. This is the <code>expected_at</code> number, not the <code>activated_at</code> number.</p>
</li>
</ul>
<h2><a class="header" href="#finalization-1" id="finalization-1">Finalization</a></h2>
<p>No finalization routine runs for this module.</p>
<h1><a class="header" href="#scheduler-module" id="scheduler-module">Scheduler Module</a></h1>
<blockquote>
<p>TODO: this section is still heavily under construction. key questions about availability cores and validator assignment are still open and the flow of the the section may be contradictory or inconsistent</p>
</blockquote>
<p>The Scheduler module is responsible for two main tasks:</p>
<ul>
<li>Partitioning validators into groups and assigning groups to parachains and parathreads.</li>
<li>Scheduling parachains and parathreads</li>
</ul>
<p>It aims to achieve these tasks with these goals in mind:</p>
<ul>
<li>It should be possible to know at least a block ahead-of-time, ideally more, which validators are going to be assigned to which parachains.</li>
<li>Parachains that have a candidate pending availability in this fork of the chain should not be assigned.</li>
<li>Validator assignments should not be gameable. Malicious cartels should not be able to manipulate the scheduler to assign themselves as desired.</li>
<li>High or close to optimal throughput of parachains and parathreads. Work among validator groups should be balanced.</li>
</ul>
<p>The Scheduler manages resource allocation using the concept of &quot;Availability Cores&quot;. There will be one availability core for each parachain, and a fixed number of cores used for multiplexing parathreads. Validators will be partitioned into groups, with the same number of groups as availability cores. Validator groups will be assigned to different availability cores over time.</p>
<p>An availability core can exist in either one of two states at the beginning or end of a block: free or occupied. A free availability core can have a parachain or parathread assigned to it for the potential to have a backed candidate included. After inclusion, the core enters the occupied state as the backed candidate is pending availability. There is an important distinction: a core is not considered occupied until it is in charge of a block pending availability, although the implementation may treat scheduled cores the same as occupied ones for brevity. A core exits the occupied state when the candidate is no longer pending availability - either on timeout or on availability. A core starting in the occupied state can move to the free state and back to occupied all within a single block, as availability bitfields are processed before backed candidates. At the end of the block, there is a possible timeout on availability which can move the core back to the free state if occupied.</p>
<pre><code class="language-text">Availability Core State Machine

              Assignment &amp;
              Backing
+-----------+              +-----------+
|           +--------------&gt;           |
|  Free     |              | Occupied  |
|           &lt;--------------+           |
+-----------+ Availability +-----------+
              or Timeout

</code></pre>
<pre><code class="language-text">Availability Core Transitions within Block

              +-----------+                |                    +-----------+
              |           |                |                    |           |
              | Free      |                |                    | Occupied  |
              |           |                |                    |           |
              +--/-----\--+                |                    +--/-----\--+
               /-       -\                 |                     /-       -\
 No Backing  /-           \ Backing        |      Availability /-           \ No availability
           /-              \               |                  /              \
         /-                 -\             |                /-                -\
  +-----v-----+         +----v------+      |         +-----v-----+        +-----v-----+
  |           |         |           |      |         |           |        |           |
  | Free      |         | Occupied  |      |         | Free      |        | Occupied  |
  |           |         |           |      |         |           |        |           |
  +-----------+         +-----------+      |         +-----|---\-+        +-----|-----+
                                           |               |    \               |
                                           |    No backing |     \ Backing      | (no change)
                                           |               |      -\            |
                                           |         +-----v-----+  \     +-----v-----+
                                           |         |           |   \    |           |
                                           |         | Free      -----+---&gt; Occupied  |
                                           |         |           |        |           |
                                           |         +-----------+        +-----------+
                                           |                 Availability Timeout
</code></pre>
<p>Validator group assignments do not need to change very quickly. The security benefits of fast rotation is redundant with the challenge mechanism in the <a href="runtime/validity.html">Validity module</a>. Because of this, we only divide validators into groups at the beginning of the session and do not shuffle membership during the session. However, we do take steps to ensure that no particular validator group has dominance over a single parachain or parathread-multiplexer for an entire session to provide better guarantees of liveness.</p>
<p>Validator groups rotate across availability cores in a round-robin fashion, with rotation occurring at fixed intervals. The i'th group will be assigned to the <code>(i+k)%n</code>'th core at any point in time, where <code>k</code> is the number of rotations that have occurred in the session, and <code>n</code> is the number of cores. This makes upcoming rotations within the same session predictable.</p>
<p>When a rotation occurs, validator groups are still responsible for distributing availability chunks for any previous cores that are still occupied and pending availability. In practice, rotation and availability-timeout frequencies should be set so this will only be the core they have just been rotated from. It is possible that a validator group is rotated onto a core which is currently occupied. In this case, the validator group will have nothing to do until the previously-assigned group finishes their availability work and frees the core or the availability process times out. Depending on if the core is for a parachain or parathread, a different timeout <code>t</code> from the <a href="runtime/../types/runtime.html#host-configuration"><code>HostConfiguration</code></a> will apply. Availability timeouts should only be triggered in the first <code>t-1</code> blocks after the beginning of a rotation.</p>
<p>Parathreads operate on a system of claims. Collators participate in auctions to stake a claim on authoring the next block of a parathread, although the auction mechanism is beyond the scope of the scheduler. The scheduler guarantees that they'll be given at least a certain number of attempts to author a candidate that is backed. Attempts that fail during the availability phase are not counted, since ensuring availability at that stage is the responsibility of the backing validators, not of the collator. When a claim is accepted, it is placed into a queue of claims, and each claim is assigned to a particular parathread-multiplexing core in advance. Given that the current assignments of validator groups to cores are known, and the upcoming assignments are predictable, it is possible for parathread collators to know who they should be talking to now and how they should begin establishing connections with as a fallback.</p>
<p>With this information, the Node-side can be aware of which parathreads have a good chance of being includable within the relay-chain block and can focus any additional resources on backing candidates from those parathreads. Furthermore, Node-side code is aware of which validator group will be responsible for that thread. If the necessary conditions are reached for core reassignment, those candidates can be backed within the same block as the core being freed.</p>
<p>Parathread claims, when scheduled onto a free core, may not result in a block pending availability. This may be due to collator error, networking timeout, or censorship by the validator group. In this case, the claims should be retried a certain number of times to give the collator a fair shot.</p>
<p>Cores are treated as an ordered list of cores and are typically referred to by their index in that list.</p>
<h2><a class="header" href="#storage-3" id="storage-3">Storage</a></h2>
<p>Utility structs:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>// A claim on authoring the next block for a given parathread.
struct ParathreadClaim(ParaId, CollatorId);

// An entry tracking a claim to ensure it does not pass the maximum number of retries.
struct ParathreadEntry {
  claim: ParathreadClaim,
  retries: u32,
}

// A queued parathread entry, pre-assigned to a core.
struct QueuedParathread {
  claim: ParathreadEntry,
  /// offset within the set of para-threads ranged `0..config.parathread_cores`.
  core_offset: u32,
}

struct ParathreadQueue {
  queue: Vec&lt;QueuedParathread&gt;,
  /// offset within the set of para-threads ranged `0..config.parathread_cores`.
  next_core_offset: u32,
}

enum CoreOccupied {
  Parathread(ParathreadEntry), // claim &amp; retries
  Parachain,
}

enum AssignmentKind {
  Parachain,
  Parathread(CollatorId, u32),
}

struct CoreAssignment {
  core: CoreIndex,
  para_id: ParaId,
  kind: AssignmentKind,
  group_idx: GroupIndex,
}
// reasons a core might be freed.
enum FreedReason {
  Concluded,
  TimedOut,
}
<span class="boring">}
</span></code></pre></pre>
<p>Storage layout:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// All the validator groups. One for each core.
ValidatorGroups: Vec&lt;Vec&lt;ValidatorIndex&gt;&gt;;
/// A queue of upcoming claims and which core they should be mapped onto.
ParathreadQueue: ParathreadQueue;
/// One entry for each availability core. Entries are `None` if the core is not currently occupied. Can be
/// temporarily `Some` if scheduled but not occupied.
/// The i'th parachain belongs to the i'th core, with the remaining cores all being
/// parathread-multiplexers.
AvailabilityCores: Vec&lt;Option&lt;CoreOccupied&gt;&gt;;
/// An index used to ensure that only one claim on a parathread exists in the queue or is
/// currently being handled by an occupied core.
ParathreadClaimIndex: Vec&lt;ParaId&gt;;
/// The block number where the session start occurred. Used to track how many group rotations have occurred.
SessionStartBlock: BlockNumber;
/// Currently scheduled cores - free but up to be occupied. Ephemeral storage item that's wiped on finalization.
Scheduled: Vec&lt;CoreAssignment&gt;, // sorted ascending by CoreIndex.
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#session-change-3" id="session-change-3">Session Change</a></h2>
<p>Session changes are the only time that configuration can change, and the <a href="runtime/configuration.html">Configuration module</a>'s session-change logic is handled before this module's. We also lean on the behavior of the <a href="runtime/inclusion.html">Inclusion module</a> which clears all its occupied cores on session change. Thus we don't have to worry about cores being occupied across session boundaries and it is safe to re-size the <code>AvailabilityCores</code> bitfield.</p>
<p>Actions:</p>
<ol>
<li>Set <code>SessionStartBlock</code> to current block number.</li>
<li>Clear all <code>Some</code> members of <code>AvailabilityCores</code>. Return all parathread claims to queue with retries un-incremented.</li>
<li>Set <code>configuration = Configuration::configuration()</code> (see <a href="runtime/../types/runtime.html#host-configuration"><code>HostConfiguration</code></a>)</li>
<li>Resize <code>AvailabilityCores</code> to have length <code>Paras::parachains().len() + configuration.parathread_cores with all</code>None` entries.</li>
<li>Compute new validator groups by shuffling using a secure randomness beacon
<ul>
<li>We need a total of <code>N = Paras::parathreads().len() + configuration.parathread_cores</code> validator groups.</li>
<li>The total number of validators <code>V</code> in the <code>SessionChangeNotification</code>'s <code>validators</code> may not be evenly divided by <code>V</code>.</li>
<li>First, we obtain &quot;shuffled validators&quot; <code>SV</code> by shuffling the validators using the <code>SessionChangeNotification</code>'s random seed.</li>
<li>The groups are selected by partitioning <code>SV</code>. The first V % N groups will have (V / N) + 1 members, while the remaining groups will have (V / N) members each.</li>
</ul>
</li>
<li>Prune the parathread queue to remove all retries beyond <code>configuration.parathread_retries</code>.
<ul>
<li>Also prune all parathread claims corresponding to de-registered parathreads.</li>
<li>all pruned claims should have their entry removed from the parathread index.</li>
<li>assign all non-pruned claims to new cores if the number of parathread cores has changed between the <code>new_config</code> and <code>old_config</code> of the <code>SessionChangeNotification</code>.</li>
<li>Assign claims in equal balance across all cores if rebalancing, and set the <code>next_core</code> of the <code>ParathreadQueue</code> by incrementing the relative index of the last assigned core and taking it modulo the number of parathread cores.</li>
</ul>
</li>
</ol>
<h2><a class="header" href="#initialization-2" id="initialization-2">Initialization</a></h2>
<ol>
<li>Schedule free cores using the <code>schedule(Vec::new())</code>.</li>
</ol>
<h2><a class="header" href="#finalization-2" id="finalization-2">Finalization</a></h2>
<p>Actions:</p>
<ol>
<li>Free all scheduled cores and return parathread claims to queue, with retries incremented.</li>
</ol>
<h2><a class="header" href="#routines-2" id="routines-2">Routines</a></h2>
<ul>
<li><code>add_parathread_claim(ParathreadClaim)</code>: Add a parathread claim to the queue.
<ul>
<li>Fails if any parathread claim on the same parathread is currently indexed.</li>
<li>Fails if the queue length is &gt;= <code>config.scheduling_lookahead * config.parathread_cores</code>.</li>
<li>The core used for the parathread claim is the <code>next_core</code> field of the <code>ParathreadQueue</code> and adding <code>Paras::parachains().len()</code> to it.</li>
<li><code>next_core</code> is then updated by adding 1 and taking it modulo <code>config.parathread_cores</code>.</li>
<li>The claim is then added to the claim index.</li>
</ul>
</li>
<li><code>schedule(Vec&lt;(CoreIndex, FreedReason)&gt;)</code>: schedule new core assignments, with a parameter indicating previously-occupied cores which are to be considered returned and why they are being returned.
<ul>
<li>All freed parachain cores should be assigned to their respective parachain</li>
<li>All freed parathread cores whose reason for freeing was <code>FreedReason::Concluded</code> should have the claim removed from the claim index.</li>
<li>All freed parathread cores whose reason for freeing was <code>FreedReason::TimedOut</code> should have the claim added to the parathread queue again without retries incremented</li>
<li>All freed parathread cores should take the next parathread entry from the queue.</li>
<li>The i'th validator group will be assigned to the <code>(i+k)%n</code>'th core at any point in time, where <code>k</code> is the number of rotations that have occurred in the session, and <code>n</code> is the total number of cores. This makes upcoming rotations within the same session predictable.</li>
</ul>
</li>
<li><code>scheduled() -&gt; Vec&lt;CoreAssignment&gt;</code>: Get currently scheduled core assignments.</li>
<li><code>occupied(Vec&lt;CoreIndex&gt;)</code>. Note that the given cores have become occupied.
<ul>
<li>Behavior undefined if any given cores were not scheduled.</li>
<li>Behavior undefined if the given cores are not sorted ascending by core index</li>
<li>This clears them from <code>Scheduled</code> and marks each corresponding <code>core</code> in the <code>AvailabilityCores</code> as occupied.</li>
<li>Since both the availability cores and the newly-occupied cores lists are sorted ascending, this method can be implemented efficiently.</li>
</ul>
</li>
<li><code>core_para(CoreIndex) -&gt; ParaId</code>: return the currently-scheduled or occupied ParaId for the given core.</li>
<li><code>group_validators(GroupIndex) -&gt; Option&lt;Vec&lt;ValidatorIndex&gt;&gt;</code>: return all validators in a given group, if the group index is valid for this session.</li>
<li><code>availability_timeout_predicate() -&gt; Option&lt;impl Fn(CoreIndex, BlockNumber) -&gt; bool&gt;</code>: returns an optional predicate that should be used for timing out occupied cores. if <code>None</code>, no timing-out should be done. The predicate accepts the index of the core, and the block number since which it has been occupied. The predicate should be implemented based on the time since the last validator group rotation, and the respective parachain and parathread timeouts, i.e. only within <code>max(config.chain_availability_period, config.thread_availability_period)</code> of the last rotation would this return <code>Some</code>.</li>
</ul>
<h1><a class="header" href="#inclusion-module" id="inclusion-module">Inclusion Module</a></h1>
<p>The inclusion module is responsible for inclusion and availability of scheduled parachains and parathreads.</p>
<h2><a class="header" href="#storage-4" id="storage-4">Storage</a></h2>
<p>Helper structs:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct AvailabilityBitfield {
  bitfield: BitVec, // one bit per core.
  submitted_at: BlockNumber, // for accounting, as meaning of bits may change over time.
}

struct CandidatePendingAvailability {
  core: CoreIndex, // availability core
  receipt: AbridgedCandidateReceipt,
  availability_votes: Bitfield, // one bit per validator.
  relay_parent_number: BlockNumber, // number of the relay-parent.
  backed_in_number: BlockNumber,
}
<span class="boring">}
</span></code></pre></pre>
<p>Storage Layout:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// The latest bitfield for each validator, referred to by index.
bitfields: map ValidatorIndex =&gt; AvailabilityBitfield;
/// Candidates pending availability.
PendingAvailability: map ParaId =&gt; CandidatePendingAvailability;

/// The current validators, by their parachain session keys.
Validators: Vec&lt;ValidatorId&gt;;

/// The current session index.
CurrentSessionIndex: SessionIndex;
<span class="boring">}
</span></code></pre></pre>
<blockquote>
<p>TODO: <code>CandidateReceipt</code> and <code>AbridgedCandidateReceipt</code> can contain code upgrades which make them very large. the code entries should be split into a different storage map with infrequent access patterns</p>
</blockquote>
<h2><a class="header" href="#session-change-4" id="session-change-4">Session Change</a></h2>
<ol>
<li>Clear out all candidates pending availability.</li>
<li>Clear out all validator bitfields.</li>
<li>Update <code>Validators</code> with the validators from the session change notification.</li>
<li>Update <code>CurrentSessionIndex</code> with the session index from the session change notification.</li>
</ol>
<h2><a class="header" href="#routines-3" id="routines-3">Routines</a></h2>
<p>All failed checks should lead to an unrecoverable error making the block invalid.</p>
<ul>
<li>
<p><code>process_bitfields(Bitfields, core_lookup: Fn(CoreIndex) -&gt; Option&lt;ParaId&gt;)</code>:</p>
<ol>
<li>check that the number of bitfields and bits in each bitfield is correct.</li>
<li>check that there are no duplicates</li>
<li>check all validator signatures.</li>
<li>apply each bit of bitfield to the corresponding pending candidate. looking up parathread cores using the <code>core_lookup</code>. Disregard bitfields that have a <code>1</code> bit for any free cores.</li>
<li>For each applied bit of each availability-bitfield, set the bit for the validator in the <code>CandidatePendingAvailability</code>'s <code>availability_votes</code> bitfield. Track all candidates that now have &gt;2/3 of bits set in their <code>availability_votes</code>. These candidates are now available and can be enacted.</li>
<li>For all now-available candidates, invoke the <code>enact_candidate</code> routine with the candidate and relay-parent number.</li>
<li>
<blockquote>
<p>TODO: pass it onwards to <code>Validity</code> module.</p>
</blockquote>
</li>
<li>Return a list of freed cores consisting of the cores where candidates have become available.</li>
</ol>
</li>
<li>
<p><code>process_candidates(BackedCandidates, scheduled: Vec&lt;CoreAssignment&gt;, group_validators: Fn(GroupIndex) -&gt; Option&lt;Vec&lt;ValidatorIndex&gt;&gt;)</code>:</p>
<ol>
<li>check that each candidate corresponds to a scheduled core and that they are ordered in the same order the cores appear in assignments in <code>scheduled</code>.</li>
<li>check that <code>scheduled</code> is sorted ascending by <code>CoreIndex</code>, without duplicates.</li>
<li>check that there is no candidate pending availability for any scheduled <code>ParaId</code>.</li>
<li>If the core assignment includes a specific collator, ensure the backed candidate is issued by that collator.</li>
<li>Ensure that any code upgrade scheduled by the candidate does not happen within <code>config.validation_upgrade_frequency</code> of <code>Paras::last_code_upgrade(para_id, true)</code>, if any, comparing against the value of <code>Paras::FutureCodeUpgrades</code> for the given para ID.</li>
<li>Check the collator's signature on the pov block.</li>
<li>check the backing of the candidate using the signatures and the bitfields, comparing against the validators assigned to the groups, fetched with the <code>group_validators</code> lookup.</li>
<li>check that the upward messages, when combined with the existing queue size, are not exceeding <code>config.max_upward_queue_count</code> and <code>config.watermark_upward_queue_size</code> parameters.</li>
<li>create an entry in the <code>PendingAvailability</code> map for each backed candidate with a blank <code>availability_votes</code> bitfield.</li>
<li>Return a <code>Vec&lt;CoreIndex&gt;</code> of all scheduled cores of the list of passed assignments that a candidate was successfully backed for, sorted ascending by CoreIndex.</li>
</ol>
</li>
<li>
<p><code>enact_candidate(relay_parent_number: BlockNumber, AbridgedCandidateReceipt)</code>:</p>
<ol>
<li>If the receipt contains a code upgrade, Call <code>Paras::schedule_code_upgrade(para_id, code, relay_parent_number + config.validationl_upgrade_delay)</code>.</li>
</ol>
<blockquote>
<p>TODO: Note that this is safe as long as we never enact candidates where the relay parent is across a session boundary. In that case, which we should be careful to avoid with contextual execution, the configuration might have changed and the para may de-sync from the host's understanding of it.</p>
</blockquote>
<ol>
<li>call <code>Router::queue_upward_messages</code> for each backed candidate.</li>
<li>Call <code>Paras::note_new_head</code> using the <code>HeadData</code> from the receipt and <code>relay_parent_number</code>.</li>
</ol>
</li>
<li>
<p><code>collect_pending</code>:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>  fn collect_pending(f: impl Fn(CoreIndex, BlockNumber) -&gt; bool) -&gt; Vec&lt;u32&gt; {
    // sweep through all paras pending availability. if the predicate returns true, when given the core index and
    // the block number the candidate has been pending availability since, then clean up the corresponding storage for that candidate.
    // return a vector of cleaned-up core IDs.
  }
<span class="boring">}
</span></code></pre></pre>
</li>
</ul>
<h1><a class="header" href="#inclusioninherent" id="inclusioninherent">InclusionInherent</a></h1>
<p>This module is responsible for all the logic carried by the <code>Inclusion</code> entry-point. This entry-point is mandatory, in that it must be invoked exactly once within every block, and it is also &quot;inherent&quot;, in that it is provided with no origin by the block author. The data within it carries its own authentication. If any of the steps within fails, the entry-point is considered as having failed and the block will be invalid.</p>
<p>This module does not have the same initialization/finalization concerns as the others, as it only requires that entry points be triggered after all modules have initialized and that finalization happens after entry points are triggered. Both of these are assumptions we have already made about the runtime's order of operations, so this module doesn't need to be initialized or finalized by the <code>Initializer</code>.</p>
<h2><a class="header" href="#storage-5" id="storage-5">Storage</a></h2>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>Included: Option&lt;()&gt;,
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#finalization-3" id="finalization-3">Finalization</a></h2>
<ol>
<li>Take (get and clear) the value of <code>Included</code>. If it is not <code>Some</code>, throw an unrecoverable error.</li>
</ol>
<h2><a class="header" href="#entry-points-1" id="entry-points-1">Entry Points</a></h2>
<ul>
<li><code>inclusion</code>: This entry-point accepts two parameters: <a href="runtime/../types/availability.html#signed-availability-bitfield"><code>Bitfields</code></a> and <a href="runtime/../type-definitions.html#backed-candidate"><code>BackedCandidates</code></a>.
<ol>
<li>The <code>Bitfields</code> are first forwarded to the <code>Inclusion::process_bitfields</code> routine, returning a set of freed cores. Provide a <code>Scheduler::core_para</code> as a core-lookup to the <code>process_bitfields</code> routine. Annotate each of these freed cores with <code>FreedReason::Concluded</code>.</li>
<li>If <code>Scheduler::availability_timeout_predicate</code> is <code>Some</code>, invoke <code>Inclusion::collect_pending</code> using it, and add timed-out cores to the free cores, annotated with <code>FreedReason::TimedOut</code>.</li>
<li>Invoke <code>Scheduler::schedule(freed)</code></li>
<li>Invoke the <code>Inclusion::process_candidates</code> routine with the parameters <code>(backed_candidates, Scheduler::scheduled(), Scheduler::group_validators)</code>.</li>
<li>Call <code>Scheduler::occupied</code> using the return value of the <code>Inclusion::process_candidates</code> call above, first sorting the list of assigned core indices.</li>
<li>If all of the above succeeds, set <code>Included</code> to <code>Some(())</code>.</li>
</ol>
</li>
</ul>
<h1><a class="header" href="#validity-module" id="validity-module">Validity Module</a></h1>
<p>After a backed candidate is made available, it is included and proceeds into an acceptance period during which validators are randomly selected to do (secondary) approval checks of the parablock. Any reports disputing the validity of the candidate will cause escalation, where even more validators are requested to check the block, and so on, until either the parablock is determined to be invalid or valid. Those on the wrong side of the dispute are slashed and, if the parablock is deemed invalid, the relay chain is rolled back to a point before that block was included.</p>
<p>However, this isn't the end of the story. We are working in a forkful blockchain environment, which carries three important considerations:</p>
<ol>
<li>For security, validators that misbehave shouldn't only be slashed on one fork, but on all possible forks. Validators that misbehave shouldn't be able to create a new fork of the chain when caught and get away with their misbehavior.</li>
<li>It is possible that the parablock being contested has not appeared on all forks.</li>
<li>If a block author believes that there is a disputed parablock on a specific fork that will resolve to a reversion of the fork, that block author is better incentivized to build on a different fork which does not include that parablock.</li>
</ol>
<p>This means that in all likelihood, there is the possibility of disputes that are started on one fork of the relay chain, and as soon as the dispute resolution process starts to indicate that the parablock is indeed invalid, that fork of the relay chain will be abandoned and the dispute will never be fully resolved on that chain.</p>
<p>Even if this doesn't happen, there is the possibility that there are two disputes underway, and one resolves leading to a reversion of the chain before the other has concluded. In this case we want to both transplant the concluded dispute onto other forks of the chain as well as the unconcluded dispute.</p>
<p>We account for these requirements by having the validity module handle two kinds of disputes.</p>
<ol>
<li>Local disputes: those contesting the validity of the current fork by disputing a parablock included within it.</li>
<li>Remote disputes: a dispute that has partially or fully resolved on another fork which is transplanted to the local fork for completion and eventual slashing.</li>
</ol>
<h2><a class="header" href="#local-disputes" id="local-disputes">Local Disputes</a></h2>
<blockquote>
<p>TODO: store all included candidate and attestations on them here. accept additional backing after the fact. accept reports based on VRF. candidate included in session S should only be reported on by validator keys from session S. trigger slashing. probably only slash for session S even if the report was submitted in session S+k because it is hard to unify identity</p>
</blockquote>
<p>One first question is to ask why different logic for local disputes is necessary. It seems that local disputes are necessary in order to create the first escalation that leads to block producers abandoning the chain and making remote disputes possible.</p>
<p>Local disputes are only allowed on parablocks that have been included on the local chain and are in the acceptance period.</p>
<p>For each such parablock, it is guaranteed by the inclusion pipeline that the parablock is available and the relevant validation code is available.</p>
<p>Disputes may occur against blocks that have happened in the session prior to the current one, from the perspective of the chain. In this case, the prior validator set is responsible for handling the dispute and to do so with their keys from the last session. This means that validator duty actually extends 1 session beyond leaving the validator set.</p>
<p>Validators self-select based on the BABE VRF output included by the block author in the block that the candidate became available.</p>
<blockquote>
<p>TODO: some more details from Jeff's paper.</p>
</blockquote>
<p>After enough validators have self-selected, the quorum will be clear and validators on the wrong side will be slashed. After concluding, the dispute will remain open for some time in order to collect further evidence of misbehaving validators, and then issue a signal in the header-chain that this fork should be abandoned along with the hash of the last ancestor before inclusion, which the chain should be reverted to, along with information about the invalid block that should be used to blacklist it from being included.</p>
<h2><a class="header" href="#remote-disputes" id="remote-disputes">Remote Disputes</a></h2>
<p>When a dispute has occurred on another fork, we need to transplant that dispute to every other fork. This poses some major challenges.</p>
<p>There are two types of remote disputes. The first is a remote roll-up of a concluded dispute. These are simply all attestations for the block, those against it, and the result of all (secondary) approval checks. A concluded remote dispute can be resolved in a single transaction as it is an open-and-shut case of a quorum of validators disagreeing with another.</p>
<p>The second type of remote dispute is the unconcluded dispute. An unconcluded remote dispute is started by any validator, using these things:</p>
<ul>
<li>A candidate</li>
<li>The session that the candidate has appeared in.</li>
<li>Backing for that candidate</li>
<li>The validation code necessary for validation of the candidate.
<blockquote>
<p>TODO: optimize by excluding in case where code appears in <code>Paras::CurrentCode</code> of this fork of relay-chain</p>
</blockquote>
</li>
<li>Secondary checks already done on that candidate, containing one or more disputes by validators. None of the disputes are required to have appeared on other chains.
<blockquote>
<p>TODO: validator-dispute could be instead replaced by a fisherman w/ bond</p>
</blockquote>
</li>
</ul>
<p>When beginning a remote dispute, at least one escalation by a validator is required, but this validator may be malicious and desires to be slashed. There is no guarantee that the para is registered on this fork of the relay chain or that the para was considered available on any fork of the relay chain.</p>
<p>So the first step is to have the remote dispute proceed through an availability process similar to the one in the <a href="runtime/inclusion.html">Inclusion Module</a>, but without worrying about core assignments or compactness in bitfields.</p>
<p>We assume that remote disputes are with respect to the same validator set as on the current fork, as BABE and GRANDPA assure that forks are never long enough to diverge in validator set.</p>
<blockquote>
<p>TODO: this is at least directionally correct. handling disputes on other validator sets seems useless anyway as they wouldn't be bonded.</p>
</blockquote>
<p>As with local disputes, the validators of the session the candidate was included on another chain are responsible for resolving the dispute and determining availability of the candidate.</p>
<p>If the candidate was not made available on another fork of the relay chain, the availability process will time out and the disputing validator will be slashed on this fork. The escalation used by the validator(s) can be replayed onto other forks to lead the wrongly-escalating validator(s) to be slashed on all other forks as well. We assume that the adversary cannot censor validators from seeing any particular forks indefinitely</p>
<blockquote>
<p>TODO: set the availability timeout for this accordingly - unlike in the inclusion pipeline we are slashing for unavailability here!</p>
</blockquote>
<p>If the availability process passes, the remote dispute is ready to be included on this chain. As with the local dispute, validators self-select based on a VRF. Given that a remote dispute is likely to be replayed across multiple forks, it is important to choose a VRF in a way that all forks processing the remote dispute will have the same one. Choosing the VRF is important as it should not allow an adversary to have control over who will be selected as a secondary approval checker.</p>
<p>After enough validator self-select, under the same escalation rules as for local disputes, the Remote dispute will conclude, slashing all those on the wrong side of the dispute. After concluding, the remote dispute remains open for a set amount of blocks to accept any further proof of additional validators being on the wrong side.</p>
<h2><a class="header" href="#slashing-and-incentivization" id="slashing-and-incentivization">Slashing and Incentivization</a></h2>
<p>The goal of the dispute is to garner a <code>&gt;2/3</code> (<code>2f + 1</code>) supermajority either in favor of or against the candidate.</p>
<p>For remote disputes, it is possible that the parablock disputed has never actually passed any availability process on any chain. In this case, validators will not be able to obtain the PoV of the parablock and there will be relatively few votes. We want to disincentivize voters claiming validity of the block from preventing it from becoming available, so we charge them a small distraction fee for wasting the others' time if the dispute does not garner a 2/3+ supermajority on either side. This fee can take the form of a small slash or a reduction in rewards.</p>
<p>When a supermajority is achieved for the dispute in either the valid or invalid direction, we will penalize non-voters either by issuing a small slash or reducing their rewards. We prevent censorship of the remaining validators by leaving the dispute open for some blocks after resolution in order to accept late votes.</p>
<h1><a class="header" href="#router-module" id="router-module">Router Module</a></h1>
<p>The Router module is responsible for storing and dispatching Upward and Downward messages from and to parachains respectively. It is intended to later handle the XCMP logic as well.</p>
<p>For each enacted block the <code>queue_upward_messages</code> entry-point is called.</p>
<h2><a class="header" href="#storage-6" id="storage-6">Storage</a></h2>
<p>Storage layout:</p>
<pre><code class="language-rust ignore">/// Messages ready to be dispatched onto the relay chain.
/// This is subject to `max_upward_queue_count` and
/// `watermark_queue_size` from `HostConfiguration`.
RelayDispatchQueues: map ParaId =&gt; Vec&lt;UpwardMessage&gt;;
/// Size of the dispatch queues. Caches sizes of the queues in `RelayDispatchQueue`.
/// First item in the tuple is the count of messages and second
/// is the total length (in bytes) of the message payloads.
RelayDispatchQueueSize: map ParaId =&gt; (u32, u32);
/// The ordered list of `ParaId`s that have a `RelayDispatchQueue` entry.
NeedsDispatch: Vec&lt;ParaId&gt;;
</code></pre>
<h2><a class="header" href="#initialization-3" id="initialization-3">Initialization</a></h2>
<p>No initialization routine runs for this module.</p>
<h2><a class="header" href="#routines-4" id="routines-4">Routines</a></h2>
<ul>
<li><code>queue_upward_messages(AbridgedCandidateReceipt)</code>:
<ol>
<li>Updates <code>NeedsDispatch</code>, and enqueues upward messages into <code>RelayDispatchQueue</code> and modifies the respective entry in <code>RelayDispatchQueueSize</code>.</li>
</ol>
</li>
</ul>
<h2><a class="header" href="#finalization-4" id="finalization-4">Finalization</a></h2>
<ol>
<li>Dispatch queued upward messages from <code>RelayDispatchQueues</code> in a FIFO order applying the <code>config.watermark_upward_queue_size</code> and <code>config.max_upward_queue_count</code> limits.</li>
</ol>
<h1><a class="header" href="#node-architecture" id="node-architecture">Node Architecture</a></h1>
<h2><a class="header" href="#design-goals" id="design-goals">Design Goals</a></h2>
<ul>
<li>Modularity: Components of the system should be as self-contained as possible. Communication boundaries between components should be well-defined and mockable. This is key to creating testable, easily reviewable code.</li>
<li>Minimizing side effects: Components of the system should aim to minimize side effects and to communicate with other components via message-passing.</li>
<li>Operational Safety: The software will be managing signing keys where conflicting messages can lead to large amounts of value to be slashed. Care should be taken to ensure that no messages are signed incorrectly or in conflict with each other.</li>
</ul>
<p>The architecture of the node-side behavior aims to embody the Rust principles of ownership and message-passing to create clean, isolatable code. Each resource should have a single owner, with minimal sharing where unavoidable.</p>
<p>Many operations that need to be carried out involve the network, which is asynchronous. This asynchrony affects all core subsystems that rely on the network as well. The approach of hierarchical state machines is well-suited to this kind of environment.</p>
<p>We introduce a hierarchy of state machines consisting of an overseer supervising subsystems, where Subsystems can contain their own internal hierarchy of jobs. This is elaborated on in the next section on Subsystems.</p>
<h1><a class="header" href="#subsystems-and-jobs" id="subsystems-and-jobs">Subsystems and Jobs</a></h1>
<p>In this section we define the notions of Subsystems and Jobs. These are guidelines for how we will employ an architecture of hierarchical state machines. We'll have a top-level state machine which oversees the next level of state machines which oversee another layer of state machines and so on. The next sections will lay out these guidelines for what we've called subsystems and jobs, since this model applies to many of the tasks that the Node-side behavior needs to encompass, but these are only guidelines and some Subsystems may have deeper hierarchies internally.</p>
<p>Subsystems are long-lived worker tasks that are in charge of performing some particular kind of work. All subsystems can communicate with each other via a well-defined protocol. Subsystems can't generally communicate directly, but must coordinate communication through an <a href="node/overseer.html">Overseer</a>, which is responsible for relaying messages, handling subsystem failures, and dispatching work signals.</p>
<p>Most work that happens on the Node-side is related to building on top of a specific relay-chain block, which is contextually known as the &quot;relay parent&quot;. We call it the relay parent to explicitly denote that it is a block in the relay chain and not on a parachain. We refer to the parent because when we are in the process of building a new block, we don't know what that new block is going to be. The parent block is our only stable point of reference, even though it is usually only useful when it is not yet a parent but in fact a leaf of the block-DAG expected to soon become a parent (because validators are authoring on top of it). Furthermore, we are assuming a forkful blockchain-extension protocol, which means that there may be multiple possible children of the relay-parent. Even if the relay parent has multiple children blocks, the parent of those children is the same, and the context in which those children is authored should be the same. The parent block is the best and most stable reference to use for defining the scope of work items and messages, and is typically referred to by its cryptographic hash.</p>
<p>Since this goal of determining when to start and conclude work relative to a specific relay-parent is common to most, if not all subsystems, it is logically the job of the Overseer to distribute those signals as opposed to each subsystem duplicating that effort, potentially being out of synchronization with each other. Subsystem A should be able to expect that subsystem B is working on the same relay-parents as it is. One of the Overseer's tasks is to provide this heartbeat, or synchronized rhythm, to the system.</p>
<p>The work that subsystems spawn to be done on a specific relay-parent is known as a job. Subsystems should set up and tear down jobs according to the signals received from the overseer. Subsystems may share or cache state between jobs.</p>
<h1><a class="header" href="#overseer" id="overseer">Overseer</a></h1>
<p>The overseer is responsible for these tasks:</p>
<ol>
<li>Setting up, monitoring, and handing failure for overseen subsystems.</li>
<li>Providing a &quot;heartbeat&quot; of which relay-parents subsystems should be working on.</li>
<li>Acting as a message bus between subsystems.</li>
</ol>
<p>The hierarchy of subsystems:</p>
<pre><code class="language-text">+--------------+      +------------------+    +--------------------+
|              |      |                  |----&gt;   Subsystem A      |
| Block Import |      |                  |    +--------------------+
|    Events    |------&gt;                  |    +--------------------+
+--------------+      |                  |----&gt;   Subsystem B      |
                      |   Overseer       |    +--------------------+
+--------------+      |                  |    +--------------------+
|              |      |                  |----&gt;   Subsystem C      |
| Finalization |------&gt;                  |    +--------------------+
|    Events    |      |                  |    +--------------------+
|              |      |                  |----&gt;   Subsystem D      |
+--------------+      +------------------+    +--------------------+

</code></pre>
<p>The overseer determines work to do based on block import events and block finalization events. It does this by keeping track of the set of relay-parents for which work is currently being done. This is known as the &quot;active leaves&quot; set. It determines an initial set of active leaves on startup based on the data on-disk, and uses events about blockchain import to update the active leaves. Updates lead to <a href="node/../types.overseer-protocol.html#overseer-signal"><code>OverseerSignal</code></a><code>::StartWork</code> and <a href="node/../types/overseer-protocol.html#overseer-signal"><code>OverseerSignal</code></a><code>::StopWork</code> being sent according to new relay-parents, as well as relay-parents to stop considering. Block import events inform the overseer of leaves that no longer need to be built on, now that they have children, and inform us to begin building on those children. Block finalization events inform us when we can stop focusing on blocks that appear to have been orphaned.</p>
<p>The overseer's logic can be described with these functions:</p>
<h2><a class="header" href="#on-startup" id="on-startup">On Startup</a></h2>
<ul>
<li>Start all subsystems</li>
<li>Determine all blocks of the blockchain that should be built on. This should typically be the head of the best fork of the chain we are aware of. Sometimes add recent forks as well.</li>
<li>For each of these blocks, send an <code>OverseerSignal::StartWork</code> to all subsystems.</li>
<li>Begin listening for block import and finality events</li>
</ul>
<h2><a class="header" href="#on-block-import-event" id="on-block-import-event">On Block Import Event</a></h2>
<ul>
<li>Apply the block import event to the active leaves. A new block should lead to its addition to the active leaves set and its parent being deactivated.</li>
<li>For any deactivated leaves send an <code>OverseerSignal::StopWork</code> message to all subsystems.</li>
<li>For any activated leaves send an <code>OverseerSignal::StartWork</code> message to all subsystems.</li>
<li>Ensure all <code>StartWork</code> messages are flushed before resuming activity as a message router.</li>
</ul>
<blockquote>
<p>TODO: in the future, we may want to avoid building on too many sibling blocks at once. the notion of a &quot;preferred head&quot; among many competing sibling blocks would imply changes in our &quot;active leaves&quot; update rules here</p>
</blockquote>
<h2><a class="header" href="#on-finalization-event" id="on-finalization-event">On Finalization Event</a></h2>
<ul>
<li>Note the height <code>h</code> of the newly finalized block <code>B</code>.</li>
<li>Prune all leaves from the active leaves which have height <code>&lt;= h</code> and are not <code>B</code>.</li>
<li>Issue <code>OverseerSignal::StopWork</code> for all deactivated leaves.</li>
</ul>
<h2><a class="header" href="#on-subsystem-failure" id="on-subsystem-failure">On Subsystem Failure</a></h2>
<p>Subsystems are essential tasks meant to run as long as the node does. Subsystems can spawn ephemeral work in the form of jobs, but the subsystems themselves should not go down. If a subsystem goes down, it will be because of a critical error that should take the entire node down as well.</p>
<h2><a class="header" href="#communication-between-subsystems" id="communication-between-subsystems">Communication Between Subsystems</a></h2>
<p>When a subsystem wants to communicate with another subsystem, or, more typically, a job within a subsystem wants to communicate with its counterpart under another subsystem, that communication must happen via the overseer. Consider this example where a job on subsystem A wants to send a message to its counterpart under subsystem B. This is a realistic scenario, where you can imagine that both jobs correspond to work under the same relay-parent.</p>
<pre><code class="language-text">     +--------+                                                           +--------+
     |        |                                                           |        |
     |Job A-1 | (sends message)                       (receives message)  |Job B-1 |
     |        |                                                           |        |
     +----|---+                                                           +----^---+
          |                  +------------------------------+                  ^
          v                  |                              |                  |
+---------v---------+        |                              |        +---------|---------+
|                   |        |                              |        |                   |
| Subsystem A       |        |       Overseer / Message     |        | Subsystem B       |
|                   --------&gt;&gt;                  Bus         --------&gt;&gt;                   |
|                   |        |                              |        |                   |
+-------------------+        |                              |        +-------------------+
                             |                              |
                             +------------------------------+
</code></pre>
<p>First, the subsystem that spawned a job is responsible for handling the first step of the communication. The overseer is not aware of the hierarchy of tasks within any given subsystem and is only responsible for subsystem-to-subsystem communication. So the sending subsystem must pass on the message via the overseer to the receiving subsystem, in such a way that the receiving subsystem can further address the communication to one of its internal tasks, if necessary.</p>
<p>This communication prevents a certain class of race conditions. When the Overseer determines that it is time for subsystems to begin working on top of a particular relay-parent, it will dispatch a <code>StartWork</code> message to all subsystems to do so, and those messages will be handled asynchronously by those subsystems. Some subsystems will receive those messsages before others, and it is important that a message sent by subsystem A after receiving <code>StartWork</code> message will arrive at subsystem B after its <code>StartWork</code> message. If subsystem A maintaned an independent channel with subsystem B to communicate, it would be possible for subsystem B to handle the side message before the <code>StartWork</code> message, but it wouldn't have any logical course of action to take with the side message - leading to it being discarded or improperly handled. Well-architectured state machines should have a single source of inputs, so that is what we do here.</p>
<p>One exception is reasonable to make for responses to requests. A request should be made via the overseer in order to ensure that it arrives after any relevant <code>StartWork</code> message. A subsystem issuing a request as a result of a <code>StartWork</code> message can safely receive the response via a side-channel for two reasons:</p>
<ol>
<li>It's impossible for a request to be answered before it arrives, it is provable that any response to a request obeys the same ordering constraint.</li>
<li>The request was sent as a result of handling a <code>StartWork</code> message. Then there is no possible future in which the <code>StartWork</code> message has not been handled upon the receipt of the response.</li>
</ol>
<p>So as a single exception to the rule that all communication must happen via the overseer we allow the receipt of responses to requests via a side-channel, which may be established for that purpose. This simplifies any cases where the outside world desires to make a request to a subsystem, as the outside world can then establish a side-channel to receive the response on.</p>
<p>It's important to note that the overseer is not aware of the internals of subsystems, and this extends to the jobs that they spawn. The overseer isn't aware of the existence or definition of those jobs, and is only aware of the outer subsystems with which it interacts. This gives subsystem implementations leeway to define internal jobs as they see fit, and to wrap a more complex hierarchy of state machines than having a single layer of jobs for relay-parent-based work. Likewise, subsystems aren't required to spawn jobs. Certain types of subsystems, such as those for shared storage or networking resources, won't perform block-based work but would still benefit from being on the Overseer's message bus. These subsystems can just ignore the overseer's signals for block-based work.</p>
<p>Furthermore, the protocols by which subsystems communicate with each other should be well-defined irrespective of the implementation of the subsystem. In other words, their interface should be distinct from their implementation. This will prevent subsystems from accessing aspects of each other that are beyond the scope of the communication boundary.</p>
<h1><a class="header" href="#backing-subsystems" id="backing-subsystems">Backing Subsystems</a></h1>
<p>The backing subsystems, when conceived as a black box, receive an arbitrary quantity of parablock candidates and associated proofs of validity from arbitrary untrusted collators. From these, they produce a bounded quantity of backable candidates which relay chain block authors may choose to include in a subsequent block.</p>
<p>In broad strokes, the flow operates like this:</p>
<ul>
<li><strong>Candidate Selection</strong> winnows the field of parablock candidates, selecting up to one of them to second.</li>
<li><strong>Candidate Backing</strong> ensures that a seconding candidate is valid, then generates the appropriate <code>Statement</code>. It also keeps track of which candidates have received the backing of a quorum of other validators.</li>
<li><strong>Statement Distribution</strong> is the networking component which ensures that all validators receive each others' statements.</li>
<li><strong>PoV Distribution</strong> is the networking component which ensures that validators considering a candidate can get the appropriate PoV.</li>
</ul>
<h1><a class="header" href="#candidate-selection" id="candidate-selection">Candidate Selection</a></h1>
<p>The Candidate Selection Subsystem is run by validators, and is responsible for interfacing with Collators to select a candidate, along with its PoV, to second during the backing process relative to a specific relay parent.</p>
<p>This subsystem includes networking code for communicating with collators, and tracks which collations specific collators have submitted. This subsystem is responsible for disconnecting and blacklisting collators who are found to have submitted invalid collations. Typically an invalid collation will be discovered by a different subsystem.</p>
<p>This subsystem is only ever interested in parablocks assigned to the particular parachain which this validator is currently handling.</p>
<p>New parablock candidates may arrive from a potentially unbounded set of collators. This subsystem chooses either 0 or 1 of them per relay parent to second. If it chooses to second a candidate, it sends an appropriate message to the <a href="node/backing/candidate-backing.html">Candidate Backing subsystem</a> to generate an appropriate <a href="node/backing/../../types/backing.html#statement-type"><code>Statement</code></a>.</p>
<p>In the event that a parablock candidate proves invalid, this subsystem will receive a message back from the Candidate Backing subsystem indicating so. If that parablock candidate originated from a collator, this subsystem will blacklist that collator. If that parablock candidate originated from a peer, this subsystem generates a report for the <a href="node/backing/../utility/misbehavior-arbitration.html">Misbehavior Arbitration subsystem</a>.</p>
<h2><a class="header" href="#protocol" id="protocol">Protocol</a></h2>
<p>Input: <a href="node/backing/../../types/overseer-protocol#candidate-selection-message"><code>CandidateSelectionMessage</code></a></p>
<p>Output:</p>
<ul>
<li>Validation requests to Validation subsystem</li>
<li><a href="node/backing/../../types/overseer-protocol.html#candidate-backing-message"><code>CandidateBackingMessage</code></a><code>::Second</code></li>
<li>Peer set manager: report peers (collators who have misbehaved)</li>
</ul>
<h2><a class="header" href="#functionality" id="functionality">Functionality</a></h2>
<p>Overarching network protocol + job for every relay-parent</p>
<blockquote>
<p>TODO The Candidate Selection network protocol is currently intentionally unspecified pending further discussion.</p>
</blockquote>
<p>Several approaches have been selected, but all have some issues:</p>
<ul>
<li>The most straightforward approach is for this subsystem to simply second the first valid parablock candidate which it sees per relay head. However, that protocol is vulnerable to a single collator which, as an attack or simply through chance, gets its block candidate to the node more often than its fair share of the time.</li>
<li>It may be possible to do some BABE-like selection algorithm to choose an &quot;Official&quot; collator for the round, but that is tricky because the collator which produces the PoV does not necessarily actually produce the block.</li>
<li>We could use relay-chain BABE randomness to generate some delay <code>D</code> on the order of 1 second, +- 1 second. The collator would then second the first valid parablock which arrives after <code>D</code>, or in case none has arrived by <code>2*D</code>, the last valid parablock which has arrived. This makes it very hard for a collator to game the system to always get its block nominated, but it reduces the maximum throughput of the system by introducing delay into an already tight schedule.</li>
<li>A variation of that scheme would be to randomly choose a number <code>I</code>, and have a fixed acceptance window <code>D</code> for parablock candidates. At the end of the period <code>D</code>, count <code>C</code>: the number of parablock candidates received. Second the one with index <code>I % C</code>. Its drawback is the same: it must wait the full <code>D</code> period before seconding any of its received candidates, reducing throughput.</li>
</ul>
<h2><a class="header" href="#candidate-selection-job" id="candidate-selection-job">Candidate Selection Job</a></h2>
<ul>
<li>Aware of validator key and assignment</li>
<li>One job for each relay-parent, which selects up to one collation for the Candidate Backing Subsystem</li>
</ul>
<h1><a class="header" href="#candidate-backing" id="candidate-backing">Candidate Backing</a></h1>
<p>The Candidate Backing subsystem ensures every parablock considered for relay block inclusion has been seconded by at least one validator, and approved by a quorum. Parablocks for which no validator will assert correctness are discarded. If the block later proves invalid, the initial backers are slashable; this gives polkadot a rational threat model during subsequent stages.</p>
<p>Its role is to produce backable candidates for inclusion in new relay-chain blocks. It does so by issuing signed <a href="node/backing/../../types/backing.html#statement-type"><code>Statement</code>s</a> and tracking received statements signed by other validators. Once enough statements are received, they can be combined into backing for specific candidates.</p>
<p>Note that though the candidate backing subsystem attempts to produce as many backable candidates as possible, it does <em>not</em> attempt to choose a single authoritative one. The choice of which actually gets included is ultimately up to the block author, by whatever metrics it may use; those are opaque to this subsystem.</p>
<p>Once a sufficient quorum has agreed that a candidate is valid, this subsystem notifies the <a href="node/backing/../utility/provisioner.html">Provisioner</a>, which in turn engages block production mechanisms to include the parablock.</p>
<h2><a class="header" href="#protocol-1" id="protocol-1">Protocol</a></h2>
<p>The <a href="node/backing/candidate-selection.html">Candidate Selection subsystem</a> is the primary source of non-overseer messages into this subsystem. That subsystem generates appropriate <a href="node/backing/../../types/overseer-protocol.html#candidate-backing-message"><code>CandidateBackingMessage</code>s</a>, and passes them to this subsystem.</p>
<p>This subsystem validates the candidates and generates an appropriate <a href="node/backing/../../types/backing.html#statement-type"><code>Statement</code></a>. All <code>Statement</code>s are then passed on to the <a href="node/backing/statement-distribution.html">Statement Distribution subsystem</a> to be gossiped to peers. When this subsystem decides that a candidate is invalid, and it was recommended to us to second by our own Candidate Selection subsystem, a message is sent to the Candidate Selection subsystem with the candidate's hash so that the collator which recommended it can be penalized.</p>
<h2><a class="header" href="#functionality-1" id="functionality-1">Functionality</a></h2>
<p>The subsystem should maintain a set of handles to Candidate Backing Jobs that are currently live, as well as the relay-parent to which they correspond.</p>
<h3><a class="header" href="#on-overseer-signal" id="on-overseer-signal">On Overseer Signal</a></h3>
<ul>
<li>If the signal is an <a href="node/backing/../../types/overseer-protocol.html#overseer-signal"><code>OverseerSignal</code></a><code>::StartWork(relay_parent)</code>, spawn a Candidate Backing Job with the given relay parent, storing a bidirectional channel with the Candidate Backing Job in the set of handles.</li>
<li>If the signal is an <a href="node/backing/../../types/overseer-protocol.html#overseer-signal"><code>OverseerSignal</code></a><code>::StopWork(relay_parent)</code>, cease the Candidate Backing Job under that relay parent, if any.</li>
</ul>
<h3><a class="header" href="#on-candidatebackingmessage" id="on-candidatebackingmessage">On <code>CandidateBackingMessage</code></a></h3>
<ul>
<li>If the message corresponds to a particular relay-parent, forward the message to the Candidate Backing Job for that relay-parent, if any is live.</li>
</ul>
<blockquote>
<p>big TODO: &quot;contextual execution&quot;</p>
<ul>
<li>At the moment we only allow inclusion of <em>new</em> parachain candidates validated by <em>current</em> validators.</li>
<li>Allow inclusion of <em>old</em> parachain candidates validated by <em>current</em> validators.</li>
<li>Allow inclusion of <em>old</em> parachain candidates validated by <em>old</em> validators.</li>
</ul>
<p>This will probably blur the lines between jobs, will probably require inter-job communication and a short-term memory of recently backable, but not backed candidates.</p>
</blockquote>
<h2><a class="header" href="#candidate-backing-job" id="candidate-backing-job">Candidate Backing Job</a></h2>
<p>The Candidate Backing Job represents the work a node does for backing candidates with respect to a particular relay-parent.</p>
<p>The goal of a Candidate Backing Job is to produce as many backable candidates as possible. This is done via signed <a href="node/backing/../../types/backing.html#statement-type"><code>Statement</code>s</a> by validators. If a candidate receives a majority of supporting Statements from the Parachain Validators currently assigned, then that candidate is considered backable.</p>
<h3><a class="header" href="#on-startup-1" id="on-startup-1">On Startup</a></h3>
<ul>
<li>Fetch current validator set, validator -&gt; parachain assignments from runtime API.</li>
<li>Determine if the node controls a key in the current validator set. Call this the local key if so.</li>
<li>If the local key exists, extract the parachain head and validation function for the parachain the local key is assigned to.</li>
</ul>
<h3><a class="header" href="#on-receiving-new-signed-statement" id="on-receiving-new-signed-statement">On Receiving New Signed Statement</a></h3>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>if let Statement::Seconded(candidate) = signed.statement {
  if candidate is unknown and in local assignment {
    spawn_validation_work(candidate, parachain head, validation function)
  }
}

// add `Seconded` statements and `Valid` statements to a quorum. If quorum reaches validator-group
// majority, send a `BlockAuthorshipProvisioning::BackableCandidate(relay_parent, Candidate, Backing)` message.
<span class="boring">}
</span></code></pre></pre>
<h3><a class="header" href="#spawning-validation-work" id="spawning-validation-work">Spawning Validation Work</a></h3>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>fn spawn_validation_work(candidate, parachain head, validation function) {
  asynchronously {
    let pov = (fetch pov block).await

    // dispatched to sub-process (OS process) pool.
    let valid = validate_candidate(candidate, validation function, parachain head, pov).await;
    if valid {
      // make PoV available for later distribution. Send data to the availability store to keep.
      // sign and dispatch `valid` statement to network if we have not seconded the given candidate.
    } else {
      // sign and dispatch `invalid` statement to network.
    }
  }
}
<span class="boring">}
</span></code></pre></pre>
<h3><a class="header" href="#fetch-pov-block" id="fetch-pov-block">Fetch Pov Block</a></h3>
<p>Create a <code>(sender, receiver)</code> pair.
Dispatch a <code>PovFetchSubsystemMessage(relay_parent, candidate_hash, sender)</code> and listen on the receiver for a response.</p>
<h3><a class="header" href="#on-receiving-candidatebackingmessage" id="on-receiving-candidatebackingmessage">On Receiving <code>CandidateBackingMessage</code></a></h3>
<ul>
<li>If the message is a <code>CandidateBackingMessage::RegisterBackingWatcher</code>, register the watcher and trigger it each time a new candidate is backable. Also trigger it once initially if there are any backable candidates at the time of receipt.</li>
<li>If the message is a <code>CandidateBackingMessage::Second</code>, sign and dispatch a <code>Seconded</code> statement only if we have not seconded any other candidate and have not signed a <code>Valid</code> statement for the requested candidate. Signing both a <code>Seconded</code> and <code>Valid</code> message is a double-voting misbehavior with a heavy penalty, and this could occur if another validator has seconded the same candidate and we've received their message before the internal seconding request.</li>
</ul>
<blockquote>
<p>TODO: send statements to Statement Distribution subsystem, handle shutdown signal from candidate backing subsystem</p>
</blockquote>
<h1><a class="header" href="#statement-distribution" id="statement-distribution">Statement Distribution</a></h1>
<p>The Statement Distribution Subsystem is responsible for distributing statements about seconded candidates between validators.</p>
<h2><a class="header" href="#protocol-2" id="protocol-2">Protocol</a></h2>
<p><code>ProtocolId</code>: <code>b&quot;stmd&quot;</code></p>
<p>Input:</p>
<ul>
<li>NetworkBridgeUpdate(update)</li>
</ul>
<p>Output:</p>
<ul>
<li>NetworkBridge::RegisterEventProducer(<code>ProtocolId</code>)</li>
<li>NetworkBridge::SendMessage(<code>[PeerId]</code>, <code>ProtocolId</code>, <code>Bytes</code>)</li>
<li>NetworkBridge::ReportPeer(PeerId, cost_or_benefit)</li>
</ul>
<h2><a class="header" href="#functionality-2" id="functionality-2">Functionality</a></h2>
<p>Implemented as a gossip protocol. Register a network event producer on startup. Handle updates to our view and peers' views. Neighbor packets are used to inform peers which chain heads we are interested in data for.</p>
<p>Statement Distribution is the only backing subsystem which has any notion of peer nodes, who are any full nodes on the network. Validators will also act as peer nodes.</p>
<p>It is responsible for signing statements that we have generated and forwarding them, and for detecting a variety of Validator misbehaviors for reporting to <a href="node/backing/../utility/misbehavior-arbitration.html">Misbehavior Arbitration</a>. During the Backing stage of the inclusion pipeline, it's the main point of contact with peer nodes, who distribute statements by validators. On receiving a signed statement from a peer, assuming the peer receipt state machine is in an appropriate state, it sends the Candidate Receipt to the <a href="node/backing/candidate-backing.html">Candidate Backing subsystem</a> to handle the validator's statement.</p>
<p>Track equivocating validators and stop accepting information from them. Forward double-vote proofs to the double-vote reporting system. Establish a data-dependency order:</p>
<ul>
<li>In order to receive a <code>Seconded</code> message we have the on corresponding chain head in our view</li>
<li>In order to receive an <code>Invalid</code> or <code>Valid</code> message we must have received the corresponding <code>Seconded</code> message.</li>
</ul>
<p>And respect this data-dependency order from our peers by respecting their views. This subsystem is responsible for checking message signatures.</p>
<p>The Statement Distribution subsystem sends statements to peer nodes and detects double-voting by validators. When validators conflict with each other or themselves, the Misbehavior Arbitration system is notified.</p>
<h2><a class="header" href="#peer-receipt-state-machine" id="peer-receipt-state-machine">Peer Receipt State Machine</a></h2>
<p>There is a very simple state machine which governs which messages we are willing to receive from peers. Not depicted in the state machine: on initial receipt of any <a href="node/backing/../../types/backing.html#signed-statement-type"><code>SignedStatement</code></a>, validate that the provided signature does in fact sign the included data. Note that each individual parablock candidate gets its own instance of this state machine; it is perfectly legal to receive a <code>Valid(X)</code> before a <code>Seconded(Y)</code>, as long as a <code>Seconded(X)</code> has been received.</p>
<p>A: Initial State. Receive <code>SignedStatement(Statement::Second)</code>: extract <code>Statement</code>, forward to Candidate Backing, proceed to B. Receive any other <code>SignedStatement</code> variant: drop it.</p>
<p>B: Receive any <code>SignedStatement</code>: check signature, forward to Candidate Backing. Receive <code>OverseerMessage::StopWork</code>: proceed to C.</p>
<p>C: Receive any message for this block: drop it.</p>
<h2><a class="header" href="#peer-knowledge-tracking" id="peer-knowledge-tracking">Peer Knowledge Tracking</a></h2>
<p>The peer receipt state machine implies that for parsimony of network resources, we should model the knowledge of our peers, and help them out. For example, let's consider a case with peers A, B, and C, validators X and Y, and candidate M. A sends us a <code>Statement::Second(M)</code> signed by X. We've double-checked it, and it's valid. While we're checking it, we receive a copy of X's <code>Statement::Second(M)</code> from <code>B</code>, along with a <code>Statement::Valid(M)</code> signed by Y.</p>
<p>Our response to A is just the <code>Statement::Valid(M)</code> signed by Y. However, we haven't heard anything about this from C. Therefore, we send it everything we have: first a copy of X's <code>Statement::Second</code>, then Y's <code>Statement::Valid</code>.</p>
<p>This system implies a certain level of duplication of messages--we received X's <code>Statement::Second</code> from both our peers, and C may experience the same--but it minimizes the degree to which messages are simply dropped.</p>
<p>And respect this data-dependency order from our peers. This subsystem is responsible for checking message signatures.</p>
<p>No jobs, <code>StartWork</code> and <code>StopWork</code> pulses are used to control neighbor packets and what we are currently accepting.</p>
<h1><a class="header" href="#pov-distribution" id="pov-distribution">PoV Distribution</a></h1>
<p>This subsystem is responsible for distributing PoV blocks. For now, unified with <a href="node/backing/statement-distribution.html">Statement Distribution subsystem</a>.</p>
<h2><a class="header" href="#protocol-3" id="protocol-3">Protocol</a></h2>
<p>Handle requests for PoV block by candidate hash and relay-parent.</p>
<h2><a class="header" href="#functionality-3" id="functionality-3">Functionality</a></h2>
<p>Implemented as a gossip system, where <code>PoV</code>s are not accepted unless we know a <code>Seconded</code> message.</p>
<blockquote>
<p>TODO: this requires a lot of cross-contamination with statement distribution even if we don't implement this as a gossip system. In a point-to-point implementation, we still have to know <em>who to ask</em>, which means tracking who's submitted <code>Seconded</code>, <code>Valid</code>, or <code>Invalid</code> statements - by validator and by peer. One approach is to have the Statement gossip system to just send us this information and then we can separate the systems from the beginning instead of combining them</p>
</blockquote>
<h1><a class="header" href="#availability-subsystems" id="availability-subsystems">Availability Subsystems</a></h1>
<p>The availability subsystems are responsible for ensuring that Proofs of Validity of backed candidates are widely available within the validator set, without requiring every node to retain a full copy. They accomplish this by broadly distributing erasure-coded chunks of the PoV, keeping track of which validator has which chunk by means of signed bitfields. They are also responsible for reassembling a complete PoV when required, e.g. when a fisherman reports a potentially invalid block.</p>
<h1><a class="header" href="#availability-distribution" id="availability-distribution">Availability Distribution</a></h1>
<p>Distribute availability erasure-coded chunks to validators.</p>
<p>After a candidate is backed, the availability of the PoV block must be confirmed by 2/3+ of all validators. Validating a candidate successfully and contributing it to being backable leads to the PoV and erasure-coding being stored in the <a href="node/availability/../utility/availability-store.html">Availability Store</a>.</p>
<h2><a class="header" href="#protocol-4" id="protocol-4">Protocol</a></h2>
<p><code>ProtocolId</code>:<code>b&quot;avad&quot;</code></p>
<p>Input:</p>
<ul>
<li>NetworkBridgeUpdate(update)</li>
</ul>
<p>Output:</p>
<ul>
<li>NetworkBridge::RegisterEventProducer(<code>ProtocolId</code>)</li>
<li>NetworkBridge::SendMessage(<code>[PeerId]</code>, <code>ProtocolId</code>, <code>Bytes</code>)</li>
<li>NetworkBridge::ReportPeer(PeerId, cost_or_benefit)</li>
<li>AvailabilityStore::QueryPoV(candidate_hash, response_channel)</li>
<li>AvailabilityStore::StoreChunk(candidate_hash, chunk_index, inclusion_proof, chunk_data)</li>
</ul>
<h2><a class="header" href="#functionality-4" id="functionality-4">Functionality</a></h2>
<p>Register on startup an event producer with  <code>NetworkBridge::RegisterEventProducer</code>.</p>
<p>For each relay-parent in our local view update, look at all backed candidates pending availability. Distribute via gossip all erasure chunks for all candidates that we have to peers.</p>
<p>We define an operation <code>live_candidates(relay_heads) -&gt; Set&lt;AbridgedCandidateReceipt&gt;</code> which returns a set of candidates a given set of relay chain heads that implies a set of candidates whose availability chunks should be currently gossiped. This is defined as all candidates pending availability in any of those relay-chain heads or any of their last <code>K</code> ancestors. We assume that state is not pruned within <code>K</code> blocks of the chain-head.</p>
<p>We will send any erasure-chunks that correspond to candidates in <code>live_candidates(peer_most_recent_view_update)</code>. Likewise, we only accept and forward messages pertaining to a candidate in <code>live_candidates(current_heads)</code>. Each erasure chunk should be accompanied by a merkle proof that it is committed to by the erasure trie root in the candidate receipt, and this gossip system is responsible for checking such proof.</p>
<p>We re-attempt to send anything live to a peer upon any view update from that peer.</p>
<p>On our view change, for all live candidates, we will check if we have the PoV by issuing a <code>QueryPoV</code> message and waiting for the response. If the query returns <code>Some</code>, we will perform the erasure-coding and distribute all messages to peers that will accept them.</p>
<p>If we are operating as a validator, we note our index <code>i</code> in the validator set and keep the <code>i</code>th availability chunk for any live candidate, as we receive it. We keep the chunk and its merkle proof in the <a href="node/availability/../utility/availability-store.html">Availability Store</a> by sending a <code>StoreChunk</code> command. This includes chunks and proofs generated as the result of a successful <code>QueryPoV</code>.</p>
<blockquote>
<p>TODO: back-and-forth is kind of ugly but drastically simplifies the pruning in the availability store, as it creates an invariant that chunks are only stored if the candidate was actually backed</p>
<p>K=3?</p>
</blockquote>
<h1><a class="header" href="#bitfield-distribution" id="bitfield-distribution">Bitfield Distribution</a></h1>
<p>Validators vote on the availability of a backed candidate by issuing signed bitfields, where each bit corresponds to a single candidate. These bitfields can be used to compactly determine which backed candidates are available or not based on a 2/3+ quorum.</p>
<h2><a class="header" href="#protocol-5" id="protocol-5">Protocol</a></h2>
<p><code>ProtocolId</code>: <code>b&quot;bitd&quot;</code></p>
<p>Input: <a href="node/availability/../../overseer-protocol.html#bitfield-distribution-message"><code>BitfieldDistributionMessage</code></a>
Output:</p>
<ul>
<li><code>NetworkBridge::RegisterEventProducer(ProtocolId)</code></li>
<li><code>NetworkBridge::SendMessage([PeerId], ProtocolId, Bytes)</code></li>
<li><code>NetworkBridge::ReportPeer(PeerId, cost_or_benefit)</code></li>
<li><code>BlockAuthorshipProvisioning::Bitfield(relay_parent, SignedAvailabilityBitfield)</code></li>
</ul>
<h2><a class="header" href="#functionality-5" id="functionality-5">Functionality</a></h2>
<p>This is implemented as a gossip system. Register a <a href="node/availability/../utility/network-bridge.html">network bridge</a> event producer on startup and track peer connection, view change, and disconnection events. Only accept bitfields relevant to our current view and only distribute bitfields to other peers when relevant to their most recent view. Check bitfield signatures in this subsystem and accept and distribute only one bitfield per validator.</p>
<p>When receiving a bitfield either from the network or from a <code>DistributeBitfield</code> message, forward it along to the block authorship (provisioning) subsystem for potential inclusion in a block.</p>
<h1><a class="header" href="#bitfield-signing" id="bitfield-signing">Bitfield Signing</a></h1>
<p>Validators vote on the availability of a backed candidate by issuing signed bitfields, where each bit corresponds to a single candidate. These bitfields can be used to compactly determine which backed candidates are available or not based on a 2/3+ quorum.</p>
<h2><a class="header" href="#protocol-6" id="protocol-6">Protocol</a></h2>
<p>Output:</p>
<ul>
<li>BitfieldDistribution::DistributeBitfield: distribute a locally signed bitfield</li>
<li>AvailabilityStore::QueryChunk(CandidateHash, validator_index, response_channel)</li>
</ul>
<h2><a class="header" href="#functionality-6" id="functionality-6">Functionality</a></h2>
<p>Upon onset of a new relay-chain head with <code>StartWork</code>, launch bitfield signing job for the head. Stop the job on <code>StopWork</code>.</p>
<h2><a class="header" href="#bitfield-signing-job" id="bitfield-signing-job">Bitfield Signing Job</a></h2>
<p>Localized to a specific relay-parent <code>r</code>
If not running as a validator, do nothing.</p>
<ul>
<li>Determine our validator index <code>i</code>, the set of backed candidates pending availability in <code>r</code>, and which bit of the bitfield each corresponds to.</li>
<li>
<blockquote>
<p>TODO: wait T time for availability distribution?</p>
</blockquote>
</li>
<li>Start with an empty bitfield. For each bit in the bitfield, if there is a candidate pending availability, query the <a href="node/availability/../utility/availability-store.html">Availability Store</a> for whether we have the availability chunk for our validator index.</li>
<li>For all chunks we have, set the corresponding bit in the bitfield.</li>
<li>Sign the bitfield and dispatch a <code>BitfieldDistribution::DistributeBitfield</code> message.</li>
</ul>
<h1><a class="header" href="#collators" id="collators">Collators</a></h1>
<p>Collators are special nodes which bridge a parachain to the relay chain. They are simultaneously full nodes of the parachain, and at least light clients of the relay chain. Their overall contribution to the system is the generation of Proofs of Validity for parachain candidates.</p>
<h1><a class="header" href="#collation-generation" id="collation-generation">Collation Generation</a></h1>
<blockquote>
<p>TODO</p>
</blockquote>
<h2><a class="header" href="#protocol-7" id="protocol-7">Protocol</a></h2>
<h2><a class="header" href="#functionality-7" id="functionality-7">Functionality</a></h2>
<h2><a class="header" href="#jobs-if-any" id="jobs-if-any">Jobs, if any</a></h2>
<h1><a class="header" href="#collation-distribution" id="collation-distribution">Collation Distribution</a></h1>
<blockquote>
<p>TODO</p>
</blockquote>
<h2><a class="header" href="#protocol-8" id="protocol-8">Protocol</a></h2>
<h2><a class="header" href="#functionality-8" id="functionality-8">Functionality</a></h2>
<h2><a class="header" href="#jobs-if-any-1" id="jobs-if-any-1">Jobs, if any</a></h2>
<h1><a class="header" href="#validity" id="validity">Validity</a></h1>
<p>The node validity subsystems exist to support the runtime <a href="node/validity/../../runtime/validity.html">Validity module</a>. Their behavior and specifications are as-yet undefined.</p>
<h1><a class="header" href="#utility-subsystems" id="utility-subsystems">Utility Subsystems</a></h1>
<p>The utility subsystems are an assortment which don't have a natural home in another subsystem collection.</p>
<h1><a class="header" href="#availability-store" id="availability-store">Availability Store</a></h1>
<p>This is a utility subsystem responsible for keeping available certain data and pruning that data.</p>
<p>The two data types:</p>
<ul>
<li>Full PoV blocks of candidates we have validated</li>
<li>Availability chunks of candidates that were backed and noted available on-chain.</li>
</ul>
<p>For each of these data we have pruning rules that determine how long we need to keep that data available.</p>
<p>PoV hypothetically only need to be kept around until the block where the data was made fully available is finalized. However, disputes can revert finality, so we need to be a bit more conservative. We should keep the PoV until a block that finalized availability of it has been finalized for 1 day.</p>
<blockquote>
<p>TODO: arbitrary, but extracting <code>acceptance_period</code> is kind of hard here...</p>
</blockquote>
<p>Availability chunks need to be kept available until the dispute period for the corresponding candidate has ended. We can accomplish this by using the same criterion as the above, plus a delay. This gives us a pruning condition of the block finalizing availability of the chunk being final for 1 day + 1 hour.</p>
<blockquote>
<p>TODO: again, concrete acceptance-period would be nicer here, but complicates things</p>
</blockquote>
<p>There is also the case where a validator commits to make a PoV available, but the corresponding candidate is never backed. In this case, we keep the PoV available for 1 hour.</p>
<blockquote>
<p>TODO: ideally would be an upper bound on how far back contextual execution is OK.</p>
</blockquote>
<p>There may be multiple competing blocks all ending the availability phase for a particular candidate. Until (and slightly beyond) finality, it will be unclear which of those is actually the canonical chain, so the pruning records for PoVs and Availability chunks should keep track of all such blocks.</p>
<h2><a class="header" href="#protocol-9" id="protocol-9">Protocol</a></h2>
<p>Input: <a href="node/utility/../../types/overseer-protocol.html#availability-store-message"><code>AvailabilityStoreMessage</code></a></p>
<h2><a class="header" href="#functionality-9" id="functionality-9">Functionality</a></h2>
<p>On <code>StartWork</code>:</p>
<ul>
<li>Note any new candidates backed in the block. Update pruning records for any stored <code>PoVBlock</code>s.</li>
<li>Note any newly-included candidates backed in the block. Update pruning records for any stored availability chunks.</li>
</ul>
<p>On block finality events:</p>
<ul>
<li>
<blockquote>
<p>TODO: figure out how we get block finality events from overseer</p>
</blockquote>
</li>
<li>Handle all pruning based on the newly-finalized block.</li>
</ul>
<p>On <code>QueryPoV</code> message:</p>
<ul>
<li>Return the PoV block, if any, for that candidate hash.</li>
</ul>
<p>On <code>QueryChunk</code> message:</p>
<ul>
<li>Determine if we have the chunk indicated by the parameters and return it and its inclusion proof via the response channel if so.</li>
</ul>
<p>On <code>StoreChunk</code> message:</p>
<ul>
<li>Store the chunk along with its inclusion proof under the candidate hash and validator index.</li>
</ul>
<h1><a class="header" href="#candidate-validation" id="candidate-validation">Candidate Validation</a></h1>
<p>This subsystem is responsible for handling candidate validation requests. It is a simple request/response server.</p>
<p>A variety of subsystems want to know if a parachain block candidate is valid. None of them care about the detailed mechanics of how a candidate gets validated, just the results. This subsystem handles those details.</p>
<h2><a class="header" href="#protocol-10" id="protocol-10">Protocol</a></h2>
<p>Input: <a href="node/utility/../../types/overseer-protocol.html#validation-request-type"><code>CandidateValidationMessage</code></a></p>
<p>Output: Validation result via the provided response side-channel.</p>
<h2><a class="header" href="#functionality-10" id="functionality-10">Functionality</a></h2>
<p>Given the hashes of a relay parent and a parachain candidate block, and either its PoV or the information with which to retrieve the PoV from the network, spawn a short-lived async job to determine whether the candidate is valid.</p>
<p>Each job follows this process:</p>
<ul>
<li>Get the full candidate from the current relay chain state</li>
<li>Check the candidate's proof
<blockquote>
<p>TODO: that's extremely hand-wavey. What does that actually entail?</p>
</blockquote>
</li>
<li>Generate either <code>Statement::Valid</code> or <code>Statement::Invalid</code>. Note that this never generates <code>Statement::Seconded</code>; Candidate Backing is the only subsystem which upgrades valid to seconded.</li>
<li>Return the statement on the provided channel.</li>
</ul>
<h1><a class="header" href="#provisioner" id="provisioner">Provisioner</a></h1>
<p>Relay chain block authorship authority is governed by BABE and is beyond the scope of the Overseer and the rest of the subsystems. That said, ultimately the block author needs to select a set of backable parachain candidates and other consensus data, and assemble a block from them. This subsystem is responsible for providing the necessary data to all potential block authors.</p>
<p>A major feature of the provisioner: this subsystem is responsible for ensuring that parachain block candidates are sufficiently available before sending them to potential block authors.</p>
<h2><a class="header" href="#provisionable-data" id="provisionable-data">Provisionable Data</a></h2>
<p>There are several distinct types of provisionable data, but they share this property in common: all should eventually be included in a relay chain block.</p>
<h3><a class="header" href="#backed-candidates" id="backed-candidates">Backed Candidates</a></h3>
<p>The block author can choose 0 or 1 backed parachain candidates per parachain; the only constraint is that each backed candidate has the appropriate relay parent. However, the choice of a backed candidate must be the block author's; the provisioner must ensure that block authors are aware of all available <a href="node/utility/../../types/backing.html#backed-candidate"><code>BackedCandidate</code>s</a>.</p>
<h3><a class="header" href="#signed-bitfields" id="signed-bitfields">Signed Bitfields</a></h3>
<p><a href="node/utility/../../types/availability.html#signed-availability-bitfield">Signed bitfields</a> are attestations from a particular validator about which candidates it believes are available.</p>
<h3><a class="header" href="#misbehavior-reports" id="misbehavior-reports">Misbehavior Reports</a></h3>
<p>Misbehavior reports are self-contained proofs of misbehavior by a validator or group of validators. For example, it is very easy to verify a double-voting misbehavior report: the report contains two votes signed by the same key, advocating different outcomes. Concretely, misbehavior reports become inherents which cause dots to be slashed.</p>
<p>Note that there is no mechanism in place which forces a block author to include a misbehavior report which it doesn't like, for example if it would be slashed by such a report. The chain's defense against this is to have a relatively long slash period, such that it's likely to encounter an honest author before the slash period expires.</p>
<h3><a class="header" href="#dispute-inherent" id="dispute-inherent">Dispute Inherent</a></h3>
<p>The dispute inherent is similar to a misbehavior report in that it is an attestation of misbehavior on the part of a validator or group of validators. Unlike a misbehavior report, it is not self-contained: resolution requires coordinated action by several validators. The canonical example of a dispute inherent involves an approval checker discovering that a set of validators has improperly approved an invalid parachain block: resolving this requires the entire validator set to re-validate the block, so that the minority can be slashed.</p>
<p>Dispute resolution is complex and is explained in substantially more detail <a href="node/utility/../../runtime/validity.html">here</a>.</p>
<blockquote>
<p>TODO: The provisioner is responsible for selecting remote disputes to replay. Let's figure out the details.</p>
</blockquote>
<h2><a class="header" href="#protocol-11" id="protocol-11">Protocol</a></h2>
<p>Input: <a href="node/utility/../../types/overseer-protocol.html#provisioner-message"><code>ProvisionerMessage</code></a>. Backed candidates come from the <a href="node/utility/../backing/candidate-backing.html">Candidate Backing subsystem</a>, signed bitfields come from the <a href="node/utility/../availability/bitfield-distribution.html">Bitfield Distribution subsystem</a>, and misbehavior reports and disputes come from the <a href="node/utility/misbehavior-arbitration.html">Misbehavior Arbitration subsystem</a>.</p>
<p>At initialization, this subsystem has no outputs. Block authors can send a <code>ProvisionerMessage::RequestBlockAuthorshipData</code>, which includes a channel over which provisionable data can be sent. All appropriate provisionable data will then be sent over this channel, as it is received.</p>
<p>Note that block authors must re-send a <code>ProvisionerMessage::RequestBlockAuthorshipData</code> for each relay parent they are interested in receiving provisionable data for.</p>
<h2><a class="header" href="#functionality-11" id="functionality-11">Functionality</a></h2>
<p>The subsystem should maintain a set of handles to Block Authorship Provisioning Jobs that are currently live.</p>
<h3><a class="header" href="#on-overseer-signal-1" id="on-overseer-signal-1">On Overseer Signal</a></h3>
<ul>
<li><code>StartWork</code>: spawn a Block Authorship Provisioning Job with the given relay parent, storing a bidirectional channel with that job.</li>
<li><code>StopWork</code>: terminate the Block Authorship Provisioning Job for the given relay parent, if any.</li>
</ul>
<h3><a class="header" href="#on-provisionermessage" id="on-provisionermessage">On <code>ProvisionerMessage</code></a></h3>
<p>Forward the message to the appropriate Block Authorship Provisioning Job, or discard if no appropriate job is currently active.</p>
<h2><a class="header" href="#block-authorship-provisioning-job" id="block-authorship-provisioning-job">Block Authorship Provisioning Job</a></h2>
<p>Maintain the set of channels to block authors. On receiving provisionable data, send a copy over each channel.</p>
<h1><a class="header" href="#network-bridge" id="network-bridge">Network Bridge</a></h1>
<p>One of the main features of the overseer/subsystem duality is to avoid shared ownership of resources and to communicate via message-passing. However, implementing each networking subsystem as its own network protocol brings a fair share of challenges.</p>
<p>The most notable challenge is coordinating and eliminating race conditions of peer connection and disconnection events. If we have many network protocols that peers are supposed to be connected on, it is difficult to enforce that a peer is indeed connected on all of them or the order in which those protocols receive notifications that peers have connected. This becomes especially difficult when attempting to share peer state across protocols. All of the Parachain-Host's gossip protocols eliminate DoS with a data-dependency on current chain heads. However, it is inefficient and confusing to implement the logic for tracking our current chain heads as well as our peers' on each of those subsystems. Having one subsystem for tracking this shared state and distributing it to the others is an improvement in architecture and efficiency.</p>
<p>One other piece of shared state to track is peer reputation. When peers are found to have provided value or cost, we adjust their reputation accordingly.</p>
<p>So in short, this Subsystem acts as a bridge between an actual network component and a subsystem's protocol.</p>
<h2><a class="header" href="#protocol-12" id="protocol-12">Protocol</a></h2>
<p>Input: <a href="node/utility/../../types/overseer-protocol.html#network-bridge-message"><code>NetworkBridgeMessage</code></a>
Output: Varying, based on registered event producers.</p>
<h2><a class="header" href="#functionality-12" id="functionality-12">Functionality</a></h2>
<p>Track a set of all Event Producers, each associated with a 4-byte protocol ID.
There are two types of network messages this sends and receives:</p>
<ul>
<li>ProtocolMessage(ProtocolId, Bytes)</li>
<li>ViewUpdate(View)</li>
</ul>
<p><code>StartWork</code> and <code>StopWork</code> determine the computation of our local view. A <code>ViewUpdate</code> is issued to each connected peer, and a <code>NetworkBridgeUpdate::OurViewChange</code> is issued for each registered event producer.</p>
<p>On <code>RegisterEventProducer</code>:</p>
<ul>
<li>Add the event producer to the set of event producers. If there is a competing entry, ignore the request.</li>
</ul>
<p>On <code>ProtocolMessage</code> arrival:</p>
<ul>
<li>If the protocol ID matches an event producer, produce the message from the <code>NetworkBridgeEvent::PeerMessage(sender, bytes)</code>, otherwise ignore and reduce peer reputation slightly</li>
<li>dispatch message via overseer.</li>
</ul>
<p>On <code>ViewUpdate</code> arrival:</p>
<ul>
<li>Do validity checks and note the most recent view update of the peer.</li>
<li>For each event producer, dispatch the result of a <code>NetworkBridgeEvent::PeerViewChange(view)</code> via overseer.</li>
</ul>
<p>On <code>ReportPeer</code> message:</p>
<ul>
<li>Adjust peer reputation according to cost or benefit provided</li>
</ul>
<p>On <code>SendMessage</code> message:</p>
<ul>
<li>Issue a corresponding <code>ProtocolMessage</code> to each listed peer with given protocol ID and bytes.</li>
</ul>
<h1><a class="header" href="#misbehavior-arbitration" id="misbehavior-arbitration">Misbehavior Arbitration</a></h1>
<p>The Misbehavior Arbitration subsystem collects reports of validator misbehavior, and slashes the stake of both misbehaving validator nodes and false accusers.</p>
<blockquote>
<p>TODO: It is not yet fully specified; that problem is postponed to a future PR.</p>
</blockquote>
<p>One policy question we've decided even so: in the event that MA has to call all validators to check some block about which some validators disagree, the minority voters all get slashed, and the majority voters all get rewarded. Validators which abstain have a minor slash penalty, but probably not in the same order of magnitude as those who vote wrong.</p>
<h1><a class="header" href="#peer-set-manager" id="peer-set-manager">Peer Set Manager</a></h1>
<blockquote>
<p>TODO</p>
</blockquote>
<h2><a class="header" href="#protocol-13" id="protocol-13">Protocol</a></h2>
<h2><a class="header" href="#functionality-13" id="functionality-13">Functionality</a></h2>
<h2><a class="header" href="#jobs-if-any-2" id="jobs-if-any-2">Jobs, if any</a></h2>
<h1><a class="header" href="#runtime-api" id="runtime-api">Runtime API</a></h1>
<p>The Runtime API subsystem is responsible for providing a single point of access to runtime state data via a set of pre-determined queries. This prevents shared ownership of a blockchain client resource by providing</p>
<h2><a class="header" href="#protocol-14" id="protocol-14">Protocol</a></h2>
<p>Input: <a href="node/utility/../../types/overseer-protocol.html#runtime-api-message"><code>RuntimeApiMessage</code></a></p>
<p>Output: None</p>
<h2><a class="header" href="#functionality-14" id="functionality-14">Functionality</a></h2>
<p>On receipt of <code>RuntimeApiMessage::Request(relay_parent, request)</code>, answer the request using the post-state of the relay_parent provided and provide the response to the side-channel embedded within the request.</p>
<blockquote>
<p>TODO Do some caching. The underlying rocksdb already has a cache of trie nodes so duplicate requests are unlikely to hit disk. Not required for functionality.</p>
</blockquote>
<h2><a class="header" href="#jobs" id="jobs">Jobs</a></h2>
<blockquote>
<p>TODO Don't limit requests based on parent hash, but limit caching. No caching should be done for any requests on relay_parents that are not live based on <code>StartWork</code> or <code>StopWork</code> messages. Maybe with some leeway for things that have just been stopped.</p>
</blockquote>
<h1><a class="header" href="#type-definitions" id="type-definitions">Type Definitions</a></h1>
<p>This section of the guide provides type definitions of various categories.</p>
<h1><a class="header" href="#candidate-types" id="candidate-types">Candidate Types</a></h1>
<p>Para candidates are some of the most common types, both within the runtime and on the Node-side.</p>
<p>In a way, this entire guide is about these candidates: how they are scheduled, constructed, backed, included, and challenged.</p>
<p>This section will describe the base candidate type, its components, and abridged counterpart.</p>
<h2><a class="header" href="#candidatereceipt" id="candidatereceipt">CandidateReceipt</a></h2>
<p>This is the base receipt type. The <code>GlobalValidationSchedule</code> and the <code>LocalValidationData</code> are technically redundant with the <code>inner.relay_parent</code>, which uniquely describes the a block in the blockchain from whose state these values are derived. The <a href="types/candidate.html#abridgedcandidatereceipt"><code>AbridgedCandidateReceipt</code></a> variant is often used instead for this reason.</p>
<p>However, the full CandidateReceipt type is useful as a means of avoiding the implicit dependency on availability of old blockchain state. In situations such as availability and approval, having the full description of the candidate within a self-contained struct is convenient.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// All data pertaining to the execution of a para candidate.
struct CandidateReceipt {
	inner: AbridgedCandidateReceipt,
	/// The global validation schedule.
	global_validation: GlobalValidationSchedule,
	/// The local validation data.
	local_validation: LocalValidationData,
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#abridgedcandidatereceipt" id="abridgedcandidatereceipt">AbridgedCandidateReceipt</a></h2>
<p>Much info in a <a href="types/candidate.html#candidatereceipt"><code>CandidateReceipt</code></a> is duplicated from the relay-chain state. When the corresponding relay-chain state is considered widely available, the Abridged Candidate Receipt should be favored.</p>
<p>Examples of situations where the state is readily available includes within the scope of work done by subsystems working on a given relay-parent, or within the logic of the runtime importing a backed candidate.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// An abridged candidate-receipt.
struct AbridgedCandidateReceipt {
	/// The ID of the para this is a candidate for.
	para_id: Id,
	/// The hash of the relay-chain block this is executed in the context of.
	relay_parent: Hash,
	/// The head-data produced as a result of execution.
	head_data: HeadData,
	/// The collator's sr25519 public key.
	collator: CollatorId,
	/// Signature on blake2-256 of components of this receipt:
	/// The parachain index, the relay parent, the head data, and the pov_hash.
	signature: CollatorSignature,
	/// The blake2-256 hash of the pov-block.
	pov_hash: Hash,
	/// Commitments made as a result of validation.
	commitments: CandidateCommitments,
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#globalvalidationschedule" id="globalvalidationschedule">GlobalValidationSchedule</a></h2>
<p>The global validation schedule comprises of information describing the global environment for para execution, as derived from a particular relay-parent. These are parameters that will apply to all parablocks executed in the context of this relay-parent.</p>
<blockquote>
<p>TODO: message queue watermarks (first upward messages, then XCMP channels)</p>
</blockquote>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// Extra data that is needed along with the other fields in a `CandidateReceipt`
/// to fully validate the candidate.
///
/// These are global parameters that apply to all candidates in a block.
struct GlobalValidationSchedule {
	/// The maximum code size permitted, in bytes.
	max_code_size: u32,
	/// The maximum head-data size permitted, in bytes.
	max_head_data_size: u32,
	/// The relay-chain block number this is in the context of.
	block_number: BlockNumber,
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#localvalidationdata" id="localvalidationdata">LocalValidationData</a></h2>
<p>This is validation data needed for execution of candidate pertaining to a specific para and relay-chain block.</p>
<p>Unlike the <a href="types/candidate.html#globalvalidationdata"><code>GlobalValidationData</code></a>, which only depends on a relay-parent, this is parameterized both by a relay-parent and a choice of one of two options:</p>
<ol>
<li>Assume that the candidate pending availability on this para at the onset of the relay-parent is included.</li>
<li>Assume that the candidate pending availability on this para at the onset of the relay-parent is timed-out.</li>
</ol>
<p>This choice can also be expressed as a choice of which parent head of the para will be built on - either optimistically on the candidate pending availability or pessimistically on the one that is surely included.</p>
<p>Para validation happens optimistically before the block is authored, so it is not possible to predict with 100% accuracy what will happen in the earlier phase of the <a href="types//runtime/inclusioninherent.html"><code>InclusionInherent</code></a> module where new availability bitfields and availability timeouts are processed. This is what will eventually define whether a candidate can be backed within a specific relay-chain block.</p>
<blockquote>
<p>TODO: determine if balance/fees are even needed here.</p>
</blockquote>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// Extra data that is needed along with the other fields in a `CandidateReceipt`
/// to fully validate the candidate. These fields are parachain-specific.
pub struct LocalValidationData {
	/// The parent head-data.
	pub parent_head: HeadData,
	/// The balance of the parachain at the moment of validation.
	pub balance: Balance,
	/// The blake2-256 hash of the validation code used to execute the candidate.
	pub validation_code_hash: Hash,
	/// Whether the parachain is allowed to upgrade its validation code.
	///
	/// This is `Some` if so, and contains the number of the minimum relay-chain
	/// height at which the upgrade will be applied, if an upgrade is signaled
	/// now.
	///
	/// A parachain should enact its side of the upgrade at the end of the first
	/// parablock executing in the context of a relay-chain block with at least this
	/// height. This may be equal to the current perceived relay-chain block height, in
	/// which case the code upgrade should be applied at the end of the signaling
	/// block.
	pub code_upgrade_allowed: Option&lt;BlockNumber&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#headdata" id="headdata">HeadData</a></h2>
<p>Head data is a type-safe abstraction around bytes (<code>Vec&lt;u8&gt;</code>) for the purposes of representing heads of parachains or parathreads.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct HeadData(Vec&lt;u8&gt;);
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#candidatecommitments" id="candidatecommitments">CandidateCommitments</a></h2>
<p>The execution and validation of parachain or parathread candidates produces a number of values which either must be committed to on the relay chain or committed to the state of the relay chain.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// Commitments made in a `CandidateReceipt`. Many of these are outputs of validation.
#[derive(PartialEq, Eq, Clone, Encode, Decode)]
#[cfg_attr(feature = &quot;std&quot;, derive(Debug, Default))]
pub struct CandidateCommitments {
	/// Fees paid from the chain to the relay chain validators.
	pub fees: Balance,
	/// Messages destined to be interpreted by the Relay chain itself.
	pub upward_messages: Vec&lt;UpwardMessage&gt;,
	/// The root of a block's erasure encoding Merkle tree.
	pub erasure_root: Hash,
	/// New validation code.
	pub new_validation_code: Option&lt;ValidationCode&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#signing-context" id="signing-context">Signing Context</a></h2>
<p>This struct provides context to signatures by combining with various payloads to localize the signature to a particular session index and relay-chain hash. Having these fields included in the signature makes misbehavior attribution much simpler.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct SigningContext {
	/// The relay-chain block hash this signature is in the context of.
	parent_hash: Hash,
	/// The session index this signature is in the context of.
	session_index: SessionIndex,
}
<span class="boring">}
</span></code></pre></pre>
<h1><a class="header" href="#backing-types" id="backing-types">Backing Types</a></h1>
<p><a href="types/candidate.html">Candidates</a> go through many phases before being considered included in a fork of the relay chain and eventually accepted.</p>
<p>These types describe the data used in the backing phase. Some are sent over the wire within subsystems, and some are simply included in the relay-chain block.</p>
<h2><a class="header" href="#validity-attestation" id="validity-attestation">Validity Attestation</a></h2>
<p>An attestation of validity for a candidate, used as part of a backing. Both the <code>Seconded</code> and <code>Valid</code> statements are considered attestations of validity. This structure is only useful where the candidate referenced is apparent.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>enum ValidityAttestation {
  /// Implicit validity attestation by issuing.
  /// This corresponds to issuance of a `Seconded` statement.
  Implicit(ValidatorSignature),
  /// An explicit attestation. This corresponds to issuance of a
  /// `Valid` statement.
  Explicit(ValidatorSignature),
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#statement-type" id="statement-type">Statement Type</a></h2>
<p>The <a href="types/../node/backing/candidate-backing.html">Candidate Backing subsystem</a> issues and signs these after candidate validation.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// A statement about the validity of a parachain candidate.
enum Statement {
  /// A statement about a new candidate being seconded by a validator. This is an implicit validity vote.
  ///
  /// The main semantic difference between `Seconded` and `Valid` comes from the fact that every validator may
  /// second only 1 candidate; this places an upper bound on the total number of candidates whose validity
  /// needs to be checked. A validator who seconds more than 1 parachain candidate per relay head is subject
  /// to slashing.
  Seconded(CandidateReceipt),
  /// A statement about the validity of a candidate, based on candidate's hash.
  Valid(Hash),
  /// A statement about the invalidity of a candidate.
  Invalid(Hash),
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#signed-statement-type" id="signed-statement-type">Signed Statement Type</a></h2>
<p>A statement, the identifier of a validator, and a signature.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// A signed statement.
struct SignedStatement {
  /// The index of the validator signing this statement.
  validator_index: ValidatorIndex,
  /// The statement itself.
  statement: Statement,
  /// The signature by the validator on the signing payload.
  signature: ValidatorSignature
}
<span class="boring">}
</span></code></pre></pre>
<p>The actual signed payload will be the SCALE encoding of <code>(compact_statement, signing_context)</code> where
<code>compact_statement</code> is a tweak of the <a href="types/backing.html#statement"><code>Statement</code></a> enum where all variants, including <code>Seconded</code>, contain only the hash of the candidate, and the <code>signing_context</code> is a <a href="types/../types/candidate.html#signing-context"><code>SigningContext</code></a>.</p>
<p>This prevents against replay attacks and allows the candidate receipt itself to be omitted when checking a signature on a <code>Seconded</code> statement in situations where the hash is known.</p>
<h2><a class="header" href="#backed-candidate" id="backed-candidate">Backed Candidate</a></h2>
<p>An <a href="types/candidate.html#abridgedcandidatereceipt"><code>AbridgedCandidateReceipt</code></a> along with all data necessary to prove its backing. This is submitted to the relay-chain to process and move along the candidate to the pending-availability stage.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct BackedCandidate {
  candidate: AbridgedCandidateReceipt,
  validity_votes: Vec&lt;ValidityAttestation&gt;,
  // the indices of validators who signed the candidate within the group. There is no need to include
  // bit for any validators who are not in the group, so this is more compact.
  // The number of bits is the number of validators in the group.
  //
  // the group should be apparent from context.
  validator_indices: BitVec,
}

struct BackedCandidates(Vec&lt;BackedCandidate&gt;); // sorted by para-id.
<span class="boring">}
</span></code></pre></pre>
<h1><a class="header" href="#availability" id="availability">Availability</a></h1>
<p>One of the key roles of validators is to ensure availability of all data necessary to validate
candidates for the duration of a challenge period. This is done via an erasure-coding of the data to keep available.</p>
<h2><a class="header" href="#signed-availability-bitfield" id="signed-availability-bitfield">Signed Availability Bitfield</a></h2>
<p>A bitfield signed by a particular validator about the availability of pending candidates.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct SignedAvailabilityBitfield {
  validator_index: ValidatorIndex,
  bitfield: Bitvec,
  signature: ValidatorSignature,
}

struct Bitfields(Vec&lt;(SignedAvailabilityBitfield)&gt;), // bitfields sorted by validator index, ascending
<span class="boring">}
</span></code></pre></pre>
<p>The signed payload is the SCALE encoding of the tuple <code>(bitfield, signing_context)</code> where <code>signing_context</code> is a <a href="types/../types/candidate.html#signing-context"><code>SigningContext</code></a>.</p>
<h2><a class="header" href="#proof-of-validity" id="proof-of-validity">Proof-of-Validity</a></h2>
<p>Often referred to as PoV, this is a type-safe wrapper around bytes (<code>Vec&lt;u8&gt;</code>) when referring to data that acts as a stateless-client proof of validity of a candidate, when used as input to the validation function of the para.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct PoV(Vec&lt;u8&gt;);
<span class="boring">}
</span></code></pre></pre>
<h1><a class="header" href="#overseer-protocol" id="overseer-protocol">Overseer Protocol</a></h1>
<p>This chapter contains message types sent to and from the overseer, and the underlying subsystem message types that are transmitted using these.</p>
<h2><a class="header" href="#overseer-signal" id="overseer-signal">Overseer Signal</a></h2>
<p>Signals from the overseer to a subsystem to request change in execution that has to be obeyed by the subsystem.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>enum OverseerSignal {
  /// Signal to start work localized to the relay-parent hash.
  StartWork(Hash),
  /// Signal to stop (or phase down) work localized to the relay-parent hash.
  StopWork(Hash),
}
<span class="boring">}
</span></code></pre></pre>
<p>All subsystems have their own message types; all of them need to be able to listen for overseer signals as well. There are currently two proposals for how to handle that with unified communication channels:</p>
<ol>
<li>Retaining the <code>OverseerSignal</code> definition above, add <code>enum FromOverseer&lt;T&gt; {Signal(OverseerSignal), Message(T)}</code>.</li>
<li>Add a generic varint to <code>OverseerSignal</code>: <code>Message(T)</code>.</li>
</ol>
<p>Either way, there will be some top-level type encapsulating messages from the overseer to each subsystem.</p>
<h2><a class="header" href="#all-messages" id="all-messages">All Messages</a></h2>
<blockquote>
<p>TODO [now]</p>
</blockquote>
<h2><a class="header" href="#availability-distribution-message" id="availability-distribution-message">Availability Distribution Message</a></h2>
<p>Messages received by the availability distribution subsystem.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>enum AvailabilityDistributionMessage {
	/// Distribute an availability chunk to other validators.
	DistributeChunk(Hash, ErasureChunk),
	/// Fetch an erasure chunk from network by candidate hash and chunk index.
	FetchChunk(Hash, u32),
	/// Event from the network.
	/// An update on network state from the network bridge.
	NetworkBridgeUpdate(NetworkBridgeEvent),
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#availability-store-message" id="availability-store-message">Availability Store Message</a></h2>
<p>Messages to and from the availability store.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>enum AvailabilityStoreMessage {
	/// Query the PoV of a candidate by hash.
	QueryPoV(Hash, ResponseChannel&lt;PoV&gt;),
	/// Query a specific availability chunk of the candidate's erasure-coding by validator index.
	/// Returns the chunk and its inclusion proof against the candidate's erasure-root.
	QueryChunk(Hash, ValidatorIndex, ResponseChannel&lt;AvailabilityChunkAndProof&gt;),
	/// Store a specific chunk of the candidate's erasure-coding by validator index, with an
	/// accompanying proof.
	StoreChunk(Hash, ValidatorIndex, AvailabilityChunkAndProof),
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#bitfield-distribution-message" id="bitfield-distribution-message">Bitfield Distribution Message</a></h2>
<p>Messages received by the bitfield distribution subsystem.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>enum BitfieldDistributionMessage {
	/// Distribute a bitfield signed by a validator to other validators.
	/// The bitfield distribution subsystem will assume this is indeed correctly signed.
	DistributeBitfield(relay_parent, SignedAvailabilityBitfield),
	/// Receive a network bridge update.
	NetworkBridgeUpdate(NetworkBridgeEvent),
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#bitfield-signing-message" id="bitfield-signing-message">Bitfield Signing Message</a></h2>
<p>Currently, the bitfield signing subsystem receives no specific messages.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// Non-instantiable message type
enum BitfieldSigningMessage { }
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#candidate-backing-message" id="candidate-backing-message">Candidate Backing Message</a></h2>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>enum CandidateBackingMessage {
  /// Registers a stream listener for updates to the set of backable candidates that could be backed
  /// in a child of the given relay-parent, referenced by its hash.
  RegisterBackingWatcher(Hash, TODO),
  /// Note that the Candidate Backing subsystem should second the given candidate in the context of the
  /// given relay-parent (ref. by hash). This candidate must be validated.
  Second(Hash, CandidateReceipt),
  /// Note a peer validator's statement about a particular candidate. Disagreements about validity must be escalated
  /// to a broader check by Misbehavior Arbitration. Agreements are simply tallied until a quorum is reached.
  Statement(Statement),
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#candidate-selection-message" id="candidate-selection-message">Candidate Selection Message</a></h2>
<p>These messages are sent to the <a href="types/../node/backing/candidate-selection.html">Candidate Selection subsystem</a> as a means of providing feedback on its outputs.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>enum CandidateSelectionMessage {
  /// We recommended a particular candidate to be seconded, but it was invalid; penalize the collator.
  Invalid(CandidateReceipt),
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#network-bridge-message" id="network-bridge-message">Network Bridge Message</a></h2>
<p>Messages received by the network bridge. This subsystem is invoked by others to manipulate access
to the low-level networking code.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>enum NetworkBridgeMessage {
	/// Register an event producer with the network bridge. This should be done early and cannot
	/// be de-registered.
	RegisterEventProducer(ProtocolId, Fn(NetworkBridgeEvent) -&gt; AllMessages),
	/// Report a cost or benefit of a peer. Negative values are costs, positive are benefits.
	ReportPeer(PeerId, cost_benefit: i32),
	/// Send a message to one or more peers on the given protocol ID.
	SendMessage([PeerId], ProtocolId, Bytes),
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#network-bridge-update" id="network-bridge-update">Network Bridge Update</a></h2>
<p>These updates are posted from the <a href="types/../node/utility/network-bridge.html">Network Bridge Subsystem</a> to other subsystems based on registered listeners.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct View(Vec&lt;Hash&gt;); // Up to `N` (5?) chain heads.

enum NetworkBridgeEvent {
	/// A peer with given ID is now connected.
	PeerConnected(PeerId, ObservedRole), // role is one of Full, Light, OurGuardedAuthority, OurSentry
	/// A peer with given ID is now disconnected.
	PeerDisconnected(PeerId),
	/// We received a message from the given peer. Protocol ID should be apparent from context.
	PeerMessage(PeerId, Bytes),
	/// The given peer has updated its description of its view.
	PeerViewChange(PeerId, View), // guaranteed to come after peer connected event.
	/// We have posted the given view update to all connected peers.
	OurViewChange(View),
}
<span class="boring">}
</span></code></pre></pre>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>enum MisbehaviorReport {
  /// These validator nodes disagree on this candidate's validity, please figure it out
  ///
  /// Most likely, the list of statments all agree except for the final one. That's not
  /// guaranteed, though; if somehow we become aware of lots of
  /// statements disagreeing about the validity of a candidate before taking action,
  /// this message should be dispatched with all of them, in arbitrary order.
  ///
  /// This variant is also used when our own validity checks disagree with others'.
  CandidateValidityDisagreement(CandidateReceipt, Vec&lt;SignedStatement&gt;),
  /// I've noticed a peer contradicting itself about a particular candidate
  SelfContradiction(CandidateReceipt, SignedStatement, SignedStatement),
  /// This peer has seconded more than one parachain candidate for this relay parent head
  DoubleVote(CandidateReceipt, SignedStatement, SignedStatement),
}
<span class="boring">}
</span></code></pre></pre>
<p>If this subsystem chooses to second a parachain block, it dispatches a <code>CandidateBackingSubsystemMessage</code>.</p>
<h2><a class="header" href="#pov-distribution-1" id="pov-distribution-1">PoV Distribution</a></h2>
<p>Messages received by the PoV Distribution subsystem are unspecified and highly tied to gossip.</p>
<blockquote>
<p>TODO</p>
</blockquote>
<h2><a class="header" href="#provisioner-message" id="provisioner-message">Provisioner Message</a></h2>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// This data becomes intrinsics or extrinsics which should be included in a future relay chain block.
enum ProvisionableData {
  /// This bitfield indicates the availability of various candidate blocks.
  Bitfield(Hash, SignedAvailabilityBitfield),
  /// The Candidate Backing subsystem believes that this candidate is valid, pending availability.
  BackedCandidate(BackedCandidate),
  /// Misbehavior reports are self-contained proofs of validator misbehavior.
  MisbehaviorReport(Hash, MisbehaviorReport),
  /// Disputes trigger a broad dispute resolution process.
  Dispute(Hash, Signature),
}

/// Message to the Provisioner.
///
/// In all cases, the Hash is that of the relay parent.
enum ProvisionerMessage {
  /// This message allows potential block authors to be kept updated with all new authorship data
  /// as it becomes available.
  RequestBlockAuthorshipData(Hash, Sender&lt;ProvisionableData&gt;),
  /// This data should become part of a relay chain block
  ProvisionableData(ProvisionableData),
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#runtime-api-message" id="runtime-api-message">Runtime API Message</a></h2>
<p>The Runtime API subsystem is responsible for providing an interface to the state of the chain's runtime.</p>
<p>Other subsystems query this data by sending these messages.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>enum RuntimeApiRequest {
	/// Get the current validator set.
	Validators(ResponseChannel&lt;Vec&lt;ValidatorId&gt;&gt;),
	/// Get a signing context for bitfields and statements.
	SigningContext(ResponseChannel&lt;SigningContext&gt;),
	/// Get the validation code for a specific para, assuming execution under given block number, and
	/// an optional block number representing an intermediate parablock executed in the context of
	/// that block.
	ValidationCode(ParaId, BlockNumber, Option&lt;BlockNumber&gt;, ResponseChannel&lt;ValidationCode&gt;),
}

enum RuntimeApiMessage {
	/// Make a request of the runtime API against the post-state of the given relay-parent.
	Request(Hash, RuntimeApiRequest),
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#statement-distribution-message" id="statement-distribution-message">Statement Distribution Message</a></h2>
<p>The Statement Distribution subsystem distributes signed statements from validators to other validators.
It receives updates from the network bridge and signed statements to share with other validators.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>enum StatementDistributionMessage {
	/// An update from the network bridge.
	NetworkBridgeUpdate(NetworkBridgeEvent),
	/// We have validated a candidate and want to share our judgment with our peers.
	/// The hash is the relay parent.
	///
	/// The statement distribution subsystem assumes that the statement should be correctly
	/// signed.
	Share(Hash, SignedStatement),
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#validation-request-type" id="validation-request-type">Validation Request Type</a></h2>
<p>Various modules request that the <a href="types/../node/utility/candidate-validation.html">Candidate Validation subsystem</a> validate a block with this message</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>enum CandidateValidationMessage {
	/// Validate a candidate with provided parameters. Returns `Err` if an only if an internal
	/// error is encountered. A bad candidate will return `Ok(false)`, while a good one will
	/// return `Ok(true)`.
	Validate(ValidationCode, CandidateReceipt, PoV, ResponseChannel&lt;Result&lt;bool&gt;&gt;),
}
<span class="boring">}
</span></code></pre></pre>
<h1><a class="header" href="#runtime" id="runtime">Runtime</a></h1>
<p>Types used within the runtime exclusively and pervasively.</p>
<h2><a class="header" href="#host-configuration" id="host-configuration">Host Configuration</a></h2>
<p>The internal-to-runtime configuration of the parachain host. This is expected to be altered only by governance procedures.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct HostConfiguration {
	/// The minimum frequency at which parachains can update their validation code.
	pub validation_upgrade_frequency: BlockNumber,
	/// The delay, in blocks, before a validation upgrade is applied.
	pub validation_upgrade_delay: BlockNumber,
	/// The acceptance period, in blocks. This is the amount of blocks after availability that validators
	/// and fishermen have to perform secondary approval checks or issue reports.
	pub acceptance_period: BlockNumber,
	/// The maximum validation code size, in bytes.
	pub max_code_size: u32,
	/// The maximum head-data size, in bytes.
	pub max_head_data_size: u32,
	/// The amount of availability cores to dedicate to parathreads.
	pub parathread_cores: u32,
	/// The number of retries that a parathread author has to submit their block.
	pub parathread_retries: u32,
	/// How often parachain groups should be rotated across parachains.
	pub parachain_rotation_frequency: BlockNumber,
	/// The availability period, in blocks, for parachains. This is the amount of blocks
	/// after inclusion that validators have to make the block available and signal its availability to
	/// the chain. Must be at least 1.
	pub chain_availability_period: BlockNumber,
	/// The availability period, in blocks, for parathreads. Same as the `chain_availability_period`,
	/// but a differing timeout due to differing requirements. Must be at least 1.
	pub thread_availability_period: BlockNumber,
	/// The amount of blocks ahead to schedule parathreads.
	pub scheduling_lookahead: u32,
	/// Total number of individual messages allowed in the parachain -&gt; relay-chain message queue.
	pub max_upward_queue_count: u32,
	/// Total size of messages allowed in the parachain -&gt; relay-chain message queue before which
	/// no further messages may be added to it. If it exceeds this then the queue may contain only
	/// a single message.
	pub watermark_upward_queue_size: u32,
}
<span class="boring">}
</span></code></pre></pre>
<h1><a class="header" href="#chain" id="chain">Chain</a></h1>
<p>Types pertaining to the relay-chain - events, structures, etc.</p>
<h2><a class="header" href="#block-import-event" id="block-import-event">Block Import Event</a></h2>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// Indicates that a new block has been added to the blockchain.
struct BlockImportEvent {
  /// The block header-hash.
  hash: Hash,
  /// The header itself.
  header: Header,
  /// Whether this block is considered the head of the best chain according to the
  /// event emitter's fork-choice rule.
  new_best: bool,
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#block-finalization-event" id="block-finalization-event">Block Finalization Event</a></h2>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// Indicates that a new block has been finalized.
struct BlockFinalizationEvent {
  /// The block header-hash.
  hash: Hash,
  /// The header of the finalized block.
  header: Header,
}
<span class="boring">}
</span></code></pre></pre>
<h1><a class="header" href="#message-types" id="message-types">Message types</a></h1>
<p>Types of messages that are passed between parachains and the relay chain: UMP, DMP, XCMP.</p>
<h2><a class="header" href="#upward-message" id="upward-message">Upward Message</a></h2>
<p>A type of messages dispatched from a parachain to the relay chain.</p>
<pre><code class="language-rust ignore">enum ParachainDispatchOrigin {
	/// As a simple `Origin::Signed`, using `ParaId::account_id` as its value. This is good when
	/// interacting with standard modules such as `balances`.
	Signed,
	/// As the special `Origin::Parachain(ParaId)`. This is good when interacting with parachain-
	/// aware modules which need to succinctly verify that the origin is a parachain.
	Parachain,
	/// As the simple, superuser `Origin::Root`. This can only be done on specially permissioned
	/// parachains.
	Root,
}

struct UpwardMessage {
	/// The origin for the message to be sent from.
	pub origin: ParachainDispatchOrigin,
	/// The message data.
	pub data: Vec&lt;u8&gt;,
}
</code></pre>
<h1><a class="header" href="#glossary" id="glossary">Glossary</a></h1>
<p>Here you can find definitions of a bunch of jargon, usually specific to the Polkadot project.</p>
<ul>
<li>BABE: (Blind Assignment for Blockchain Extension). The algorithm validators use to safely extend the Relay Chain. See <a href="https://wiki.polkadot.network/docs/en/learn-consensus">the Polkadot wiki</a> for more information.</li>
<li>Backable Candidate: A Parachain Candidate which is backed by a majority of validators assigned to a given parachain.</li>
<li>Backed Candidate: A Backable Candidate noted in a relay-chain block</li>
<li>Backing: A set of statements proving that a Parachain Candidate is backable.</li>
<li>Collator: A node who generates Proofs-of-Validity (PoV) for blocks of a specific parachain.</li>
<li>Extrinsic: An element of a relay-chain block which triggers a specific entry-point of a runtime module with given arguments.</li>
<li>GRANDPA: (Ghost-based Recursive ANcestor Deriving Prefix Agreement). The algorithm validators use to guarantee finality of the Relay Chain.</li>
<li>Inclusion Pipeline: The set of steps taken to carry a Parachain Candidate from authoring, to backing, to availability and full inclusion in an active fork of its parachain.</li>
<li>Module: A component of the Runtime logic, encapsulating storage, routines, and entry-points.</li>
<li>Module Entry Point: A recipient of new information presented to the Runtime. This may trigger routines.</li>
<li>Module Routine: A piece of code executed within a module by block initialization, closing, or upon an entry point being triggered. This may execute computation, and read or write storage.</li>
<li>Node: A participant in the Polkadot network, who follows the protocols of communication and connection to other nodes. Nodes form a peer-to-peer network topology without a central authority.</li>
<li>Parachain Candidate, or Candidate: A proposed block for inclusion into a parachain.</li>
<li>Parablock: A block in a parachain.</li>
<li>Parachain: A constituent chain secured by the Relay Chain's validators.</li>
<li>Parachain Validators: A subset of validators assigned during a period of time to back candidates for a specific parachain</li>
<li>Parathread: A parachain which is scheduled on a pay-as-you-go basis.</li>
<li>Proof-of-Validity (PoV): A stateless-client proof that a parachain candidate is valid, with respect to some validation function.</li>
<li>Relay Parent: A block in the relay chain, referred to in a context where work is being done in the context of the state at this block.</li>
<li>Runtime: The relay-chain state machine.</li>
<li>Runtime Module: See Module.</li>
<li>Runtime API: A means for the node-side behavior to access structured information based on the state of a fork of the blockchain.</li>
<li>Secondary Checker: A validator who has been randomly selected to perform secondary approval checks on a parablock which is pending approval.</li>
<li>Subsystem: A long-running task which is responsible for carrying out a particular category of work.</li>
<li>Validator: Specially-selected node in the network who is responsible for validating parachain blocks and issuing attestations about their validity.</li>
<li>Validation Function: A piece of Wasm code that describes the state-transition function of a parachain.</li>
</ul>
<p>Also of use is the <a href="https://substrate.dev/docs/en/overview/glossary">Substrate Glossary</a>.</p>
<h1><a class="header" href="#further-reading" id="further-reading">Further Reading</a></h1>
<ul>
<li>Polkadot Wiki on Consensus: <a href="https://wiki.polkadot.network/docs/en/learn-consensus">https://wiki.polkadot.network/docs/en/learn-consensus</a></li>
<li>Polkadot Runtime Spec: <a href="https://github.com/w3f/polkadot-spec/tree/spec-rt-anv-vrf-gen-and-announcement/runtime-spec">https://github.com/w3f/polkadot-spec/tree/spec-rt-anv-vrf-gen-and-announcement/runtime-spec</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        
        
        
        <script type="text/javascript">
            window.playpen_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
